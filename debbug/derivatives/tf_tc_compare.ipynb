{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch as tc\n",
    "import numpy as np\n",
    "import os \n",
    "from tf_utils.utils import *\n",
    "from tc_utils.utils import *\n",
    "import tensorflow.keras as tk\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from torch.autograd import grad as tcgrad\n",
    "npDTYPE = np.float32\n",
    "tcDTYPE = tc.float32\n",
    "tfDTYPE = tf.float32\n",
    "\n",
    "npDTYPE = np.float64\n",
    "tcDTYPE = tc.float64\n",
    "tfDTYPE = tf.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch(tensors):\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        new_tensor = tc.tensor(tensor.numpy(), requires_grad=True)\n",
    "        new_tensors.append(new_tensor)\n",
    "    return new_tensors\n",
    "\n",
    "def tcVariable(tensor):\n",
    "    return tc.autograd.Variable(tc.tensor(tensor))\n",
    "\n",
    "def compute_relative_vectors(v1, v2):\n",
    "    relative_vectors = v1.unsqueeze(2) - v2.unsqueeze(1)\n",
    "    return relative_vectors\n",
    "\n",
    "# def compare_tensors(ptorch, tflow):\n",
    "#     ptorch = ptorch.detach().numpy()\n",
    "#     tflow = tflow.numpy()\n",
    "#     mask = np.isclose(ptorch, tflow, rtol=0.0, atol=1e-4).astype(np.float32)\n",
    "#     return np.mean(mask)\n",
    "\n",
    "def compare_tensors(ptorch, tflow):\n",
    "    ptorch = ptorch.detach().numpy()\n",
    "    tflow = tflow.numpy()\n",
    "    ptorch = np.reshape(ptorch, tflow.shape)\n",
    "    x = np.mean(np.abs(ptorch - tflow))\n",
    "    return x\n",
    "\n",
    "def compare_tf_tensors(tflow1, tflow2,rtol=1e-05, atol=1e-08): #\n",
    "    tflow1 = tflow1.numpy()\n",
    "    tflow2 = tflow2.numpy()\n",
    "    mask = np.isclose(tflow1, tflow2).astype(np.float32)\n",
    "    return np.mean(mask) \n",
    "\n",
    "\n",
    "\n",
    "ct = compare_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fns\n",
    "\n",
    "def linear(w, a, n_samples, n_conv, flow=True):\n",
    "    if flow:\n",
    "        a = tf.concat((a, tf.ones((n_samples, n_conv, 1), dtype=tfDTYPE)), axis=-1)\n",
    "        out = tf.tanh(a @ w)\n",
    "        return out\n",
    "    else:\n",
    "        a = tc.cat((a, tc.ones((n_samples, n_conv, 1), dtype=tcDTYPE)), dim=-1)\n",
    "        out = tc.tanh(a @ w)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def env(inputs, ae_vecs, env_w, env_sigma, env_pi, n_samples, n_spins, flow=True):\n",
    "    if flow:\n",
    "        inputs = tf.concat((inputs, tf.ones((n_samples, n_spins, 1), dtype=tfDTYPE)), axis=-1)\n",
    "        factor = tf.einsum('njf,kifs->njkis', inputs, env_w)\n",
    "        \n",
    "        exponent = tf.einsum('njmv,kimvc->njkimc', ae_vecs, env_sigma)\n",
    "        exponential = tf.exp(-tf.norm(exponent, axis=-1))\n",
    "\n",
    "        exp = tf.einsum('njkim,kims->njkis', exponential, env_pi)\n",
    "\n",
    "        output = factor * exp\n",
    "        output = tf.transpose(output, perm=(0, 2, 3, 1, 4))  # ij ordering doesn't matter / slight numerical diff\n",
    "\n",
    "        return tf.squeeze(output, -1)  \n",
    "        \n",
    "    else:\n",
    "        inputs = tc.cat((inputs, tc.ones((n_samples, n_spins, 1), dtype=tcDTYPE)), dim=-1)\n",
    "        factor = tc.einsum('njf,kifs->njkis', inputs, env_w)\n",
    "        \n",
    "        exponent = tc.einsum('njmv,kimvc->njkimc', ae_vecs, env_sigma)\n",
    "        exponential = tc.exp(-tc.norm(exponent, dim=-1))\n",
    "\n",
    "        exp = tc.einsum('njkim,kims->njkis', exponential, env_pi)\n",
    "\n",
    "        output = factor * exp\n",
    "        output = output.permute((0, 2, 3, 1, 4))  # ij ordering doesn't matter / slight numerical diff\n",
    "\n",
    "        return output.squeeze(-1)  \n",
    "    \n",
    "def compute_inputs(r_electrons, n_samples, ae_vectors, n_atoms, n_electrons, full_pairwise, flow=True):\n",
    "    # r_atoms: (n_atoms, 3)\n",
    "    # r_electrons: (n_samples, n_electrons, 3)\n",
    "    # ae_vectors: (n_samples, n_electrons, n_atoms, 3)\n",
    "    if flow:\n",
    "        ae_distances = tf.norm(ae_vectors, axis=-1, keepdims=True)\n",
    "        single_inputs = tf.concat((ae_vectors, ae_distances), axis=-1)\n",
    "        single_inputs = tf.reshape(single_inputs, (-1, n_electrons, 4*n_atoms))\n",
    "\n",
    "        re1 = tf.expand_dims(r_electrons, 2)\n",
    "        re2 = tf.transpose(re1, perm=(0, 2, 1, 3))\n",
    "        ee_vectors = re1 - re2\n",
    "\n",
    "        # ** full pairwise\n",
    "        if full_pairwise:\n",
    "            # eye_mask = tf.expand_dims(tf.expand_dims(tf.eye(n_electrons, dtype=tf.bool), 0), -1)\n",
    "            # tmp = tf.where(eye_mask, 1., tf.norm(ee_vectors, keepdims=True, axis=-1))\n",
    "            # ee_distances = tf.where(eye_mask, tf.zeros_like(eye_mask, dtype=tf.float32), tmp)\n",
    "            ee_vectors = tf.reshape(ee_vectors, (-1, n_electrons**2, 3))\n",
    "            ee_distances = safe_norm(ee_vectors)\n",
    "            pairwise_inputs = tf.concat((ee_vectors, ee_distances), axis=-1)\n",
    "            # pairwise_inputs = tf.reshape(pairwise_inputs, (-1, n_electrons**2, 4))\n",
    "        else:\n",
    "            # ** partial pairwise\n",
    "            mask = tf.eye(n_electrons, dtype=tf.bool)\n",
    "            mask = ~tf.tile(tf.expand_dims(tf.expand_dims(mask, 0), 3), (n_samples, 1, 1, 3))\n",
    "\n",
    "            ee_vectors = tf.boolean_mask(ee_vectors, mask)\n",
    "            ee_vectors = tf.reshape(ee_vectors, (-1, n_electrons**2 - n_electrons, 3))\n",
    "            ee_distances = tf.norm(ee_vectors, axis=-1, keepdims=True)\n",
    "\n",
    "            pairwise_inputs = tf.concat((ee_vectors, ee_distances), axis=-1)\n",
    "\n",
    "        return single_inputs, pairwise_inputs\n",
    "    else:\n",
    "        ae_distances = tc.norm(ae_vectors, dim=-1, keepdim=True)\n",
    "        single_inputs = tc.cat((ae_vectors, ae_distances), dim=-1)\n",
    "        single_inputs = single_inputs.view((-1, n_electrons, 4 * n_atoms))\n",
    "\n",
    "        re1 = r_electrons.unsqueeze(2)\n",
    "        re2 = re1.permute((0, 2, 1, 3))\n",
    "        ee_vectors = re1 - re2\n",
    "\n",
    "        # ** full pairwise\n",
    "        if full_pairwise:\n",
    "            # eye_mask = tf.expand_dims(tf.expand_dims(tf.eye(n_electrons, dtype=tf.bool), 0), -1)\n",
    "            # tmp = tf.where(eye_mask, 1., tf.norm(ee_vectors, keepdims=True, axis=-1))\n",
    "            # ee_distances = tf.where(eye_mask, tf.zeros_like(eye_mask, dtype=tf.float32), tmp)\n",
    "            ee_vectors = ee_vectors.view((-1, n_electrons ** 2, 3))\n",
    "            ee_distances = tc.norm(ee_vectors, dim=-1, keepdim=True) + 1e-16\n",
    "            pairwise_inputs = tc.cat((ee_vectors, ee_distances), dim=-1)\n",
    "            # pairwise_inputs = tf.reshape(pairwise_inputs, (-1, n_electrons**2, 4))\n",
    "        else:\n",
    "            print('NOT IMPLEMENTED')\n",
    "\n",
    "        return single_inputs, pairwise_inputs\n",
    "    \n",
    "def compute_ae_vectors(r_atoms, r_electrons, flow):\n",
    "    # ae_vectors (n_samples, n_electrons, n_atoms, 3)\n",
    "    if flow:\n",
    "        r_atoms = tf.expand_dims(r_atoms, 1)\n",
    "        r_electrons = tf.expand_dims(r_electrons, 2)\n",
    "        ae_vectors = r_electrons - r_atoms\n",
    "    else:\n",
    "        r_atoms = r_atoms.unsqueeze(1)\n",
    "        r_electrons = r_electrons.unsqueeze(2)\n",
    "        ae_vectors = r_electrons - r_atoms\n",
    "    return ae_vectors\n",
    "\n",
    "\n",
    "def init(in_dim, weight_shape, out_dim, env_init=0.0):\n",
    "    if env_init == 0.0:\n",
    "        minval = np.maximum(-1., -(6/(in_dim+out_dim))**0.5)\n",
    "        maxval = np.minimum(1., (6/(in_dim+out_dim))**0.5)\n",
    "        weights = np.random.uniform(size=weight_shape, low=minval, high=maxval)\n",
    "    else:\n",
    "        weights = np.random.uniform(size=weight_shape, low=-env_init, high=env_init)\n",
    "    return weights.astype(npDTYPE)\n",
    "\n",
    "def apbi(w, axis=None):\n",
    "    if axis is None:\n",
    "        shape = (1, *w.shape[1:])\n",
    "        b = np.ones(shape).astype(npDTYPE)\n",
    "        return np.concatenate((w, b), axis=0)\n",
    "    shape = (*w.shape[:axis], 1, *w.shape[axis+1:])\n",
    "    b = np.ones(shape).astype(npDTYPE)\n",
    "    return np.concatenate((w, b), axis=axis)\n",
    "\n",
    "\n",
    "# model architecture\n",
    "full_pairwise = True\n",
    "n_atoms = 1\n",
    "n_electrons = 4\n",
    "n_spin_up = 2\n",
    "n_spin_down = n_electrons - n_spin_up\n",
    "n_pairwise = n_electrons**2\n",
    "if not full_pairwise:\n",
    "    n_pairwise -= n_electrons\n",
    "\n",
    "nf_single_in = 4 * n_atoms\n",
    "nf_hidden_single = 128\n",
    "nf_pairwise_in = 4\n",
    "nf_hidden_pairwise = 16\n",
    "nf_intermediate_single = 3*nf_hidden_single + 2*nf_hidden_pairwise\n",
    "\n",
    "n_determinants = 8\n",
    "\n",
    "env_init = 1.\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "# params # in_dim, weight_shape, out_dim, env_init=0.0\n",
    "s_stream_0 = apbi(init(nf_single_in, (nf_single_in, nf_hidden_single), nf_hidden_single))\n",
    "p_stream_0 = apbi(init(nf_pairwise_in, (nf_pairwise_in, nf_hidden_pairwise), nf_hidden_pairwise))\n",
    "\n",
    "s_stream_1 = apbi(init(nf_intermediate_single, (nf_intermediate_single, nf_hidden_single), nf_hidden_single))\n",
    "p_stream_1 = apbi(init(nf_hidden_pairwise, (nf_hidden_pairwise, nf_hidden_pairwise), nf_hidden_pairwise))\n",
    "\n",
    "s_stream_2 = apbi(init(nf_intermediate_single, (nf_intermediate_single, nf_hidden_single), nf_hidden_single))\n",
    "\n",
    "env_w_up = apbi(init(nf_hidden_single, (n_determinants, n_spin_up, nf_hidden_single, 1), 1), axis=2)\n",
    "env_sigma_up  = init(3, (n_determinants, n_spin_up, n_atoms, 3, 3), 3, env_init=env_init)\n",
    "env_pi_up = init(n_atoms, (n_determinants, n_spin_up, n_atoms, 1), 1, env_init=env_init)\n",
    "\n",
    "env_w_down = apbi(init(nf_hidden_single, (n_determinants, n_spin_down, nf_hidden_single, 1), 1), axis=2)\n",
    "env_sigma_down = init(3, (n_determinants, n_spin_down, n_atoms, 3, 3), 3, env_init=env_init)\n",
    "env_pi_down = init(n_atoms, (n_determinants, n_spin_down, n_atoms, 1), 1, env_init=env_init)\n",
    "\n",
    "w_final = init(n_determinants, (1, n_determinants, 1, 1), 1, env_init=env_init/n_determinants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.custom_gradient\n",
    "def safe_norm_grad(x, norm):\n",
    "    # x : (n, ne**2, 3)\n",
    "    # norm : (n, ne**2, 1)\n",
    "    g = x / norm\n",
    "    g = tf.where(tf.math.is_nan(g), tf.zeros_like(g), g)\n",
    "    cache = (x, norm)\n",
    "\n",
    "    def grad_grad(dy):\n",
    "        x, norm = cache\n",
    "        x = tf.expand_dims(x, -1)  # (n, ne**2, 3, 1)\n",
    "        xx = x * tf.transpose(x, perm=(0, 1, 3, 2))  # cross terms\n",
    "        inv_norm = tf.tile(1. / norm, (1, 1, 3))  # (n, ne**2, 3) inf where the ee terms are same e\n",
    "        norm_diag = tf.linalg.diag(inv_norm) # (n, ne**2, 3, 3) # diagonal where the basis vector is the same\n",
    "        gg = norm_diag - xx / tf.expand_dims(norm, -1)**3\n",
    "        gg = tf.reduce_sum(gg, axis=-1)\n",
    "        gg = tf.where(tf.math.is_nan(gg), tf.zeros_like(gg), gg)\n",
    "        tf.debugging.check_numerics(gg, 'gg')\n",
    "        tf.debugging.check_numerics(dy, 'dy')\n",
    "        return dy*gg, None\n",
    "\n",
    "    return g, grad_grad\n",
    "\n",
    "@tf.custom_gradient\n",
    "def safe_norm(x):\n",
    "    norm = tf.norm(x, keepdims=True, axis=-1)\n",
    "    def grad(dy):\n",
    "        g = safe_norm_grad(x, norm)\n",
    "        return dy*g\n",
    "    return norm, grad\n",
    "\n",
    "class Mixer(tk.Model):\n",
    "    def __init__(self, n_electrons, n_single_features, n_pairwise, n_pairwise_features, n_spin_up, n_spin_down, full_pairwise):\n",
    "        super(Mixer, self).__init__()\n",
    "\n",
    "        self.n_spin_up = float(n_spin_up)\n",
    "        self.n_spin_down = float(n_spin_down)\n",
    "\n",
    "        tmp1 = tf.ones((1, n_spin_up, n_single_features), dtype=tf.bool)\n",
    "        tmp2 = tf.zeros((1, n_spin_down, n_single_features), dtype=tf.bool)\n",
    "        self.spin_up_mask = tf.concat((tmp1, tmp2), 1)\n",
    "        self.spin_down_mask = ~self.spin_up_mask\n",
    "\n",
    "        if full_pairwise:\n",
    "            self.pairwise_spin_up_mask, self.pairwise_spin_down_mask = \\\n",
    "                generate_pairwise_masks_full(n_electrons, n_pairwise, n_spin_up, n_spin_down, n_pairwise_features)\n",
    "        else:\n",
    "            self.pairwise_spin_up_mask, self.pairwise_spin_down_mask = \\\n",
    "                generate_pairwise_masks(n_electrons, n_pairwise, n_spin_up, n_spin_down, n_pairwise_features)\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, single, pairwise, n_samples, n_electrons):\n",
    "        # single (n_samples, n_electrons, n_single_features)\n",
    "        # pairwise (n_samples, n_electrons, n_pairwise_features)\n",
    "        spin_up_mask = tf.tile(self.spin_up_mask, (n_samples, 1, 1))\n",
    "        spin_down_mask = tf.tile(self.spin_down_mask, (n_samples, 1, 1))\n",
    "\n",
    "        # --- Single summations\n",
    "        replace = tf.zeros_like(single, dtype=tfDTYPE)\n",
    "        # up\n",
    "        sum_spin_up = tf.where(spin_up_mask, single, replace)\n",
    "        sum_spin_up = tf.reduce_sum(sum_spin_up, 1, keepdims=True) / self.n_spin_up\n",
    "        sum_spin_up = tf.tile(sum_spin_up, (1, n_electrons, 1))\n",
    "        # down\n",
    "        sum_spin_down = tf.where(spin_down_mask, single, replace)\n",
    "        sum_spin_down = tf.reduce_sum(sum_spin_down, 1, keepdims=True) / self.n_spin_down\n",
    "        sum_spin_down = tf.tile(sum_spin_down, (1, n_electrons, 1))\n",
    "\n",
    "        # --- Pairwise summations\n",
    "        sum_pairwise = tf.tile(tf.expand_dims(pairwise, 1), (1, n_electrons, 1, 1))\n",
    "        replace = tf.zeros_like(sum_pairwise, dtype=tfDTYPE)\n",
    "        # up\n",
    "        sum_pairwise_up = tf.where(self.pairwise_spin_up_mask, sum_pairwise, replace)\n",
    "        sum_pairwise_up = tf.reduce_sum(sum_pairwise_up, 2) / self.n_spin_up\n",
    "        # down\n",
    "        sum_pairwise_down = tf.where(self.pairwise_spin_down_mask, sum_pairwise, replace)\n",
    "        sum_pairwise_down = tf.reduce_sum(sum_pairwise_down, 2) / self.n_spin_down\n",
    "\n",
    "        features = tf.concat((single, sum_spin_up, sum_spin_down, sum_pairwise_up, sum_pairwise_down), 2)\n",
    "        return features\n",
    "    \n",
    "\n",
    "class fermi_tf():\n",
    "    def __init__(self, r_atoms):\n",
    "        self.r_atoms = r_atoms\n",
    "        \n",
    "        self.s0 = tf.Variable(s_stream_0)\n",
    "        self.p0 = tf.Variable(p_stream_0)\n",
    "        self.m0 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s1 = tf.Variable(s_stream_1)\n",
    "        self.p1 = tf.Variable(p_stream_1)\n",
    "        self.m1 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s2 = tf.Variable(s_stream_2)\n",
    "        \n",
    "        self.env_w_up = tf.Variable(env_w_up)\n",
    "        self.env_sigma_up = tf.Variable(env_sigma_up)\n",
    "        self.env_pi_up = tf.Variable(env_pi_up)\n",
    "        \n",
    "        self.env_w_down = tf.Variable(env_w_down)\n",
    "        self.env_sigma_down = tf.Variable(env_sigma_down)\n",
    "        self.env_pi_down = tf.Variable(env_pi_down)\n",
    "        \n",
    "        self.w_final = tf.Variable(w_final)\n",
    "        \n",
    "    def __call__(self, samples):\n",
    "        n_samples = samples.shape[0]\n",
    "        ae_vectors = compute_ae_vectors(self.r_atoms, samples, flow=True)\n",
    "        single, pairwise = compute_inputs(samples, n_samples, ae_vectors, n_atoms, n_electrons, full_pairwise)\n",
    "        \n",
    "        # streams \n",
    "        s0 = linear(self.s0, single, n_samples, n_electrons, flow=True)\n",
    "        p0 = linear(self.p0, pairwise, n_samples, n_pairwise, flow=True)\n",
    "        s0m = self.m0(s0, p0, n_samples, n_electrons)\n",
    "        \n",
    "        s1 = linear(self.s1, s0m, n_samples, n_electrons, flow=True)\n",
    "        p1 = linear(self.p1, p0, n_samples, n_pairwise, flow=True)\n",
    "        s1m = self.m1(s1, p1, n_samples, n_electrons)\n",
    "        \n",
    "        s2 = linear(self.s2, s1m, n_samples, n_electrons, flow=True)\n",
    "        \n",
    "        # env inputs\n",
    "        ae_vectors_up, ae_vectors_down = tf.split(ae_vectors, [n_spin_up, n_spin_down], axis=1)\n",
    "        inputs_up, inputs_down = tf.split(s2, [n_spin_up, n_spin_down], axis=1)\n",
    "        \n",
    "        # env_w 'njf,kifs->nkjis'\n",
    "        # env_sigma 'njmv,kimvc->nkjimc'\n",
    "        # env_pi 'njkim,kims->nkjis'\n",
    "        up_dets = env(inputs_up, ae_vectors_up, self.env_w_up, self.env_sigma_up, self.env_pi_up, n_samples, n_spin_up)\n",
    "        down_dets = env(inputs_down, ae_vectors_down, self.env_w_down, self.env_sigma_down, self.env_pi_down, n_samples, n_spin_down)\n",
    "        \n",
    "        log_psi, _, _, _ = log_abs_sum_det(up_dets, down_dets, self.w_final)\n",
    "        \n",
    "        return log_psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class fermi_tc():\n",
    "    def __init__(self, r_atoms):\n",
    "        self.r_atoms = r_atoms\n",
    "        \n",
    "        self.s0 = tcVariable(s_stream_0)\n",
    "        self.p0 = tcVariable(p_stream_0)\n",
    "        self.m0 = tcMixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s1 = tcVariable(s_stream_1)\n",
    "        self.p1 = tcVariable(p_stream_1)\n",
    "        self.m1 = tcMixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s2 = tcVariable(s_stream_2)\n",
    "        \n",
    "        self.env_w_up = tcVariable(env_w_up)\n",
    "        self.env_sigma_up = tcVariable(env_sigma_up)\n",
    "        self.env_pi_up = tcVariable(env_pi_up)\n",
    "        \n",
    "        self.env_w_down = tcVariable(env_w_down)\n",
    "        self.env_sigma_down = tcVariable(env_sigma_down)\n",
    "        self.env_pi_down = tcVariable(env_pi_down)\n",
    "        \n",
    "        self.w_final = tcVariable(w_final)\n",
    "        \n",
    "    def __call__(self, samples):\n",
    "        n_samples = samples.shape[0]\n",
    "        ae_vectors = compute_ae_vectors(self.r_atoms, samples, flow=False)\n",
    "        single, pairwise = compute_inputs(samples, n_samples, ae_vectors, n_atoms, n_electrons, full_pairwise, flow=False)\n",
    "        \n",
    "        # streams \n",
    "        s0 = linear(self.s0, single, n_samples, n_electrons, flow=False)\n",
    "        p0 = linear(self.p0, pairwise, n_samples, n_pairwise, flow=False)\n",
    "        s0m = self.m0(s0, p0, n_samples, n_electrons)\n",
    "        \n",
    "        s1 = linear(self.s1, s0m, n_samples, n_electrons, flow=False)\n",
    "        p1 = linear(self.p1, p0, n_samples, n_pairwise, flow=False)\n",
    "        s1m = self.m1(s1, p1, n_samples, n_electrons)\n",
    "        \n",
    "        s2 = linear(self.s2, s1m, n_samples, n_electrons, flow=False)\n",
    "        \n",
    "        # env inputs\n",
    "        ae_vectors_up, ae_vectors_down = ae_vectors.split([n_spin_up, n_spin_down], dim=1)\n",
    "        inputs_up, inputs_down = s2.split([n_spin_up, n_spin_down], dim=1)\n",
    "        \n",
    "        # env_w 'njf,kifs->nkjis'\n",
    "        # env_sigma 'njmv,kimvc->nkjimc'\n",
    "        # env_pi 'njkim,kims->nkjis'\n",
    "        up_dets = env(inputs_up, ae_vectors_up, self.env_w_up, self.env_sigma_up, self.env_pi_up, n_samples, n_spin_up, flow=False)\n",
    "        down_dets = env(inputs_down, ae_vectors_down, self.env_w_down, self.env_sigma_down, self.env_pi_down, n_samples, n_spin_down, flow=False)\n",
    "        \n",
    "        log_psi = (self.w_final.squeeze(-1).squeeze(-1) * up_dets.det() * down_dets.det()).sum(1).abs().log()\n",
    "\n",
    "        return log_psi\n",
    "\n",
    "\n",
    "\n",
    "class tcMixer():\n",
    "    def __init__(self, n_electrons, n_single_features, n_pairwise, n_pairwise_features, n_spin_up, n_spin_down, full_pairwise):\n",
    "        super(tcMixer, self).__init__()\n",
    "\n",
    "        self.n_spin_up = float(n_spin_up)\n",
    "        self.n_spin_down = float(n_spin_down)\n",
    "\n",
    "        tmp1 = tc.ones((1, n_spin_up, n_single_features), dtype=tc.bool)\n",
    "        tmp2 = tc.zeros((1, n_spin_down, n_single_features), dtype=tc.bool)\n",
    "        self.spin_up_mask = tc.cat((tmp1, tmp2), dim=1)\n",
    "        self.spin_down_mask = ~self.spin_up_mask\n",
    "\n",
    "        if full_pairwise:\n",
    "            self.pairwise_spin_up_mask, self.pairwise_spin_down_mask = \\\n",
    "                tc_generate_pairwise_masks_full(n_electrons, n_pairwise, n_spin_up, n_spin_down, n_pairwise_features)\n",
    "        else:\n",
    "            self.pairwise_spin_up_mask, self.pairwise_spin_down_mask = \\\n",
    "                tc_generate_pairwise_masks(n_electrons, n_pairwise, n_spin_up, n_spin_down, n_pairwise_features)\n",
    "\n",
    "    # @tf.function\n",
    "    def __call__(self, single, pairwise, n_samples, n_electrons):\n",
    "        # single (n_samples, n_electrons, n_single_features)\n",
    "        # pairwise (n_samples, n_electrons, n_pairwise_features)\n",
    "        spin_up_mask = self.spin_up_mask.repeat((n_samples, 1, 1))\n",
    "        spin_down_mask = self.spin_down_mask.repeat((n_samples, 1, 1))\n",
    "\n",
    "        # --- Single summations\n",
    "        replace = tc.zeros_like(single, dtype=tcDTYPE)\n",
    "        # up\n",
    "        sum_spin_up = tc.where(spin_up_mask, single, replace)\n",
    "        sum_spin_up = sum_spin_up.sum(1, keepdim=True) / self.n_spin_up\n",
    "        sum_spin_up = sum_spin_up.repeat((1, n_electrons, 1))\n",
    "        # down\n",
    "        sum_spin_down = tc.where(spin_down_mask, single, replace)\n",
    "        sum_spin_down = sum_spin_down.sum(1, keepdim=True) / self.n_spin_down\n",
    "        sum_spin_down = sum_spin_down.repeat((1, n_electrons, 1))\n",
    "\n",
    "        # --- Pairwise summations\n",
    "        sum_pairwise = pairwise.unsqueeze(1).repeat((1, n_electrons, 1, 1))\n",
    "        replace = tc.zeros_like(sum_pairwise, dtype=tcDTYPE)\n",
    "        # up\n",
    "        sum_pairwise_up = tc.where(self.pairwise_spin_up_mask, sum_pairwise, replace)\n",
    "        sum_pairwise_up = sum_pairwise_up.sum(2) / self.n_spin_up\n",
    "        # down\n",
    "        sum_pairwise_down = tc.where(self.pairwise_spin_down_mask, sum_pairwise, replace)\n",
    "        sum_pairwise_down = sum_pairwise_down.sum(2) / self.n_spin_down\n",
    "\n",
    "        features = tc.cat((single, sum_spin_up, sum_spin_down, sum_pairwise_up, sum_pairwise_down), dim=2)\n",
    "        return features\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxiwelian/anaconda3/envs/aqua/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:253: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "6.297184995673888e-15\n"
     ]
    }
   ],
   "source": [
    "samples = np.random.normal(size=(n_samples, n_electrons, 3)).astype(npDTYPE)\n",
    "r_atoms = np.zeros(shape=(1,3)).astype(npDTYPE)\n",
    "\n",
    "ftf = fermi_tf(tf.convert_to_tensor(r_atoms, dtype=tfDTYPE))\n",
    "psi = ftf(tf.convert_to_tensor(samples, dtype=tfDTYPE))\n",
    "\n",
    "ftc = fermi_tc(tc.tensor(r_atoms))\n",
    "tcpsi = ftc(tc.tensor(samples))\n",
    "\n",
    "print(compare_tensors(tcpsi, psi))\n",
    "\n",
    "# for x, y in zip(tf.reshape(psi, (-1,)), tcpsi.view(-1)):\n",
    "#     print(x.numpy(), y.numpy(), x.numpy() - y.numpy())\n",
    "\n",
    "# def get_jacobian(net, x, noutputs):\n",
    "#     x = x.squeeze()\n",
    "#     n = x.size()[0]\n",
    "#     x = x.repeat(noutputs, 1, 1)\n",
    "#     x.requires_grad_(True)\n",
    "#     y = net(x)\n",
    "#     y.backward(tc.eye(noutputs))\n",
    "#     y.backward()\n",
    "#     return x\n",
    "\n",
    "# z = get_jacobian(ftc, tc.tensor(samples), n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tflaplacian(model, r_electrons):\n",
    "    n_electrons = r_electrons.shape[1]\n",
    "    r_electrons = tf.reshape(r_electrons, (-1, n_electrons*3))\n",
    "    r_s = [r_electrons[..., i] for i in range(r_electrons.shape[-1])]\n",
    "    with tf.GradientTape(True) as g:\n",
    "        [g.watch(r) for r in r_s]\n",
    "        r_electrons = tf.stack(r_s, -1)\n",
    "        r_electrons = tf.reshape(r_electrons, (-1, n_electrons, 3))\n",
    "        with tf.GradientTape(True) as gg:\n",
    "            gg.watch(r_electrons)\n",
    "            log_phi = model(r_electrons)\n",
    "        dlogphi_dr = gg.gradient(log_phi, r_electrons)\n",
    "        dlogphi_dr = tf.reshape(dlogphi_dr, (-1, n_electrons*3))\n",
    "        grads = [dlogphi_dr[..., i] for i in range(dlogphi_dr.shape[-1])]\n",
    "    d2logphi_dr2 = tf.stack([g.gradient(grad, r) for grad, r in zip(grads, r_s)], -1)\n",
    "    return dlogphi_dr**2, d2logphi_dr2\n",
    "\n",
    "\n",
    "\n",
    "# split before\n",
    "def tclaplacian(model, samples):\n",
    "    gs = []\n",
    "    ggs = []\n",
    "    for sample in samples:\n",
    "#         sample = tc.tensor(sample, requires_grad=True).view((1,n_electrons,3))\n",
    "        s_vars = [tc.tensor(s, requires_grad=True, dtype=tcDTYPE) for s in sample.reshape(-1)]\n",
    "        sample = tc.tensor(s_vars, requires_grad=True).view((1, n_electrons, 3))\n",
    "        \n",
    "        log_psi = ftc(sample)\n",
    "#         print(log_psi.shape)\n",
    "        grads = [tcgrad(log_psi, s, retain_graph=True, create_graph=True)[0] for s in s_vars]\n",
    "        grads_grads = [tc.grad(g, s)[0] for s in s_vars]\n",
    "\n",
    "#         grad_grad = []\n",
    "#         for g, s in zip(grad, sample):\n",
    "#             gg = tcgrad(g, s)[0]\n",
    "#             grad_grad.append(gg)\n",
    "#         grad_grad = tc.tensor(grad_grad).view((1, n_electrons, 3))\n",
    "#         gs.append(grad)\n",
    "#         ggs.append(grad_grad)\n",
    "        \n",
    "    gs = tc.cat(gs, dim=0)\n",
    "    ggs = tc.cat(ggs, dim=0)\n",
    "    \n",
    "    return gs**2, ggs\n",
    "\n",
    "# use single element of grad\n",
    "def tclaplacian(model, samples):\n",
    "    gs = []\n",
    "    ggs = []\n",
    "    for sample in samples:\n",
    "        sample = tc.tensor(sample, requires_grad=True).view((1,n_electrons,3))\n",
    "\n",
    "        log_psi = ftc(sample)\n",
    "\n",
    "        g = tcgrad(log_psi, sample, retain_graph=True, create_graph=True)[0]\n",
    "        grads = g.view(-1)\n",
    "        grads_grads = tc.cat([tcgrad(g, sample, retain_graph=True)[0].view(-1, 1) for g in grads], dim=1)\n",
    "        \n",
    "        tmp = tc.tensor(grads_grads)\n",
    "\n",
    "        diag = tc.diag(tmp).view(1, n_electrons, 3)\n",
    "        gg = diag.view(1, n_electrons, 3)\n",
    "        gs.append(g)\n",
    "        ggs.append(gg)\n",
    "        \n",
    "    gs = tc.cat(gs, dim=0)\n",
    "    ggs = tc.cat(ggs, dim=0)\n",
    "    \n",
    "    return gs**2, ggs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.375077994860476e-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxiwelian/anaconda3/envs/aqua/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0270995505490266e-09\n",
      "nan\n",
      "torch.Size([5, 4, 3])\n",
      "torch.Size([5, 4, 3])\n",
      "tf.Tensor(\n",
      "[ -19.29961634  -87.18604379   -0.81376779   -4.90160414 -132.53382403\n",
      "  -11.76403391   -0.99668069   -0.80846922   -0.78383973   -1.88864622\n",
      "   -1.68921441   -0.39341681], shape=(12,), dtype=float64) tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], dtype=torch.float64)\n",
      "tf.Tensor(\n",
      "[ -673.18811932  -251.43500673  -809.75039198 -1266.04850088\n",
      "   -94.8649032  -1482.00867262   -12.41136838   -87.36419847\n",
      "    -3.36276821     5.85261893   -54.23747558   -11.01805754], shape=(12,), dtype=float64) tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], dtype=torch.float64)\n",
      "tf.Tensor(\n",
      "[  0.20252174  -9.02504047  -0.69286701  -5.40744415  -2.38037426\n",
      " -18.45727703  -3.24288052  -0.48214612  -0.81017975  -0.42474699\n",
      "  -7.97809512  -8.04122579], shape=(12,), dtype=float64) tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], dtype=torch.float64)\n",
      "tf.Tensor(\n",
      "[-9.01392666e+00 -7.91580983e+00 -1.81792136e-02 -7.76990823e-01\n",
      "  3.65587648e-01 -3.09821909e-01 -1.32339871e+01 -3.80227955e-02\n",
      " -3.27641542e+00 -1.82556081e+01 -8.83620219e+00 -4.14219613e+00], shape=(12,), dtype=float64) tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], dtype=torch.float64)\n",
      "tf.Tensor(\n",
      "[ -2958.47393028  -2110.20278697 -33908.09157159  -4298.56509753\n",
      "  -4594.33024414  -1255.86082105 -29966.145653    -2231.43809258\n",
      "     34.50736769  -3563.45620772   -164.34753104  -2159.99752247], shape=(12,), dtype=float64) tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "samples = np.random.normal(size=(n_samples, n_electrons, 3)).astype(npDTYPE)\n",
    "r_atoms = np.zeros(shape=(1,3)).astype(npDTYPE)\n",
    "\n",
    "ftf = fermi_tf(tf.convert_to_tensor(r_atoms, dtype=tfDTYPE))\n",
    "psi = ftf(tf.convert_to_tensor(samples, dtype=tfDTYPE))\n",
    "\n",
    "ftc = fermi_tc(tc.tensor(r_atoms))\n",
    "\n",
    "tcpsi = ftc(tc.tensor(samples))\n",
    "\n",
    "print(compare_tensors(tcpsi, psi))\n",
    "\n",
    "tfgs, tfggs = tflaplacian(ftf, samples)\n",
    "\n",
    "gs, ggs = tclaplacian(ftc, samples)\n",
    "\n",
    "print(compare_tensors(gs, tfgs))\n",
    "print(compare_tensors(ggs, tfggs))\n",
    "print(gs.shape)\n",
    "print(ggs.shape)\n",
    "\n",
    "\n",
    "for g1, g2 in zip(tfggs, ggs):\n",
    "    print(g1, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.diag(tc.tensor([[1,2], [4, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class tc_safe_norm_2nd(tc.autograd.Function):\n",
    "   @staticmethod\n",
    "    def forward(ctx, tensor):\n",
    "        norm = tc.norm(tensor, dim=-1, keepdim=True\n",
    "        grad = tensor / norm\n",
    "        grad = tf.where(tc.isnan(grad), tc.zeros_like(grad), grad)\n",
    "        ctx.save_for_backward([norm, tensor])\n",
    "        return norm\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        norm, tensor = ctx.saved_tensors\n",
    "        \n",
    "        return grad \n",
    "    \n",
    "class tc_safe_norm(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor):\n",
    "        x = tc.norm(tensor, dim=-1, keepdim=True)\n",
    "        ctx.save_for_backward(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# n_samples = 100\n",
    "# n_k = 10\n",
    "# ndim = 4\n",
    "# A = tf.random.normal((n_samples, n_k, ndim, ndim), dtype=DTYPE)\n",
    "# B = tf.random.normal((n_samples, n_k, ndim, ndim), dtype=DTYPE)\n",
    "# W = tf.random.normal((1,n_k,1,1), dtype=DTYPE)\n",
    "\n",
    "# A_tc, B_tc, W_tc = convert_to_torch([A, B, W])\n",
    "\n",
    "# with tf.GradientTape(True) as g:\n",
    "#     g.watch(A)\n",
    "#     g.watch(W)\n",
    "#     g.watch(B)\n",
    "#     with tf.GradientTape(True) as gg:\n",
    "#         gg.watch(A)\n",
    "#         gg.watch(B)\n",
    "#         gg.watch(W)\n",
    "#         log_psi,_, _, _ = log_abs_sum_det(A, B, W)\n",
    "#     grad_1_A = gg.gradient(log_psi, A)\n",
    "#     grad_1_B = gg.gradient(log_psi, B)\n",
    "#     grad_1_W = gg.gradient(log_psi, W)\n",
    "# grad_2_A = g.gradient(grad_1_A, A)\n",
    "\n",
    "# log_psi_tc = fn(A_tc, B_tc, W_tc)\n",
    "\n",
    "# # first order\n",
    "# grad_1_Atc = tcgrad(log_psi_tc.sum(), A_tc, retain_graph=True, create_graph=True)[0]\n",
    "# grad_1_Btc = tcgrad(log_psi_tc.sum(), B_tc, retain_graph=True, create_graph=True)[0]\n",
    "# grad_1_Wtc = tcgrad(log_psi_tc.sum(), W_tc, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "# # second order\n",
    "# grad_2_Atc = tcgrad(grad_1_Atc.sum(), A_tc)[0]\n",
    "\n",
    "# # validation\n",
    "# validate_log_psi = compare_tensors(log_psi_tc, log_psi)\n",
    "# print(validate_log_psi)\n",
    "# validate_gradW = compare_tensors(grad_1_Wtc, grad_1_W)\n",
    "# print(validate_gradW)\n",
    "\n",
    "# validate_grad_1_A = compare_tensors(grad_1_Atc, grad_1_A) \n",
    "# print(validate_grad_1_A)\n",
    "\n",
    "# validate_grad_2_A = compare_tensors(grad_2_Atc, grad_2_A)\n",
    "# print(validate_grad_2_A)\n",
    "\n",
    "# # if validate_grad1 < 1:\n",
    "# #     log_psi_tc = log_psi_tc.reshape(-1)\n",
    "# #     log_psi = tf.reshape(log_psi, (-1,))\n",
    "# #     for el1, el2 in zip(log_psi_tc, log_psi):\n",
    "# #         print(abs(el1.detach().numpy()-el2.numpy()))\n",
    "# #     print(log_psi_tc, log_psi)\n",
    "    \n",
    "# #     print('determinants: ', A_tc.det(), B_tc.det())\n",
    "# # print(compare_tensors(grad_1_tc, grad_1))\n",
    "\n",
    "# # print('B: ', ct(B_tc, B))\n",
    "# # print('ae_vectors:', compare_tensors(ae_vectors_tc, ae_vectors))\n",
    "# # print('exponential: ', compare_tensors(expo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 100\n",
    "# n_k = 10\n",
    "# n_f = 20\n",
    "# n_up = 2\n",
    "# n_e = 4\n",
    "# n_down = n_e - n_up\n",
    "# n_atom = 1 \n",
    "\n",
    "# ### Parameters\n",
    "# # First\n",
    "# r_atoms = tf.zeros((n_samples, n_atom, 3), dtype=DTYPE)\n",
    "# re = tf.random.normal((n_samples, n_e, 3), dtype=DTYPE) #* 100.\n",
    "# w1 = tf.random.normal((n_f, 3), dtype=DTYPE)\n",
    "# # up\n",
    "# w_up = tf.random.normal((n_k, n_f, n_up), dtype=DTYPE)\n",
    "# b_up = tf.random.normal((n_k, n_up, 1), dtype=DTYPE)\n",
    "# Sigma_up = tf.random.normal((n_k, n_up, n_atom, 3, 3), dtype=DTYPE)\n",
    "# Pi_up = tf.random.normal((n_k, n_up, n_atom), dtype=DTYPE)\n",
    "# # down\n",
    "# w_down = tf.random.normal((n_k, n_f, n_down), dtype=DTYPE)\n",
    "# b_down = tf.random.normal((n_k, n_down, 1), dtype=DTYPE)\n",
    "# Sigma_down = tf.random.normal((n_k, n_down, n_atom, 3, 3), dtype=DTYPE)\n",
    "# Pi_down = tf.random.normal((n_k, n_down, n_atom), dtype=DTYPE)\n",
    "# # Weights\n",
    "# W = tf.random.normal((1, n_k, 1, 1), dtype=DTYPE)\n",
    "\n",
    "# with tf.GradientTape(True) as g:\n",
    "#     g.watch(W)\n",
    "#     g.watch(re)\n",
    "#     with tf.GradientTape(True) as gg:\n",
    "#         gg.watch(re)\n",
    "#         gg.watch(W)\n",
    "#         A, B = fermi_tf(r_atoms, re, w1,  w_up, b_up, Sigma_up, Pi_up, \\\n",
    "#                          w_down, b_down, Sigma_down, Pi_down)\n",
    "#         log_psi,_ = log_abs_sum_det(A, B, W)\n",
    "\n",
    "#     grad_1_A = gg.gradient(log_psi, A)\n",
    "#     grad_1_B = gg.gradient(log_psi, B)\n",
    "#     grad_1_W = gg.gradient(log_psi, W)\n",
    "#     grad_1_re = gg.gradient(log_psi, re)\n",
    "#     grads_1_re = tf.reshape(grad_1_re, (-1, n_e*3))\n",
    "#     grads_1_re = [grads_1_re[..., i] for i in range(grads_1_re.shape[-1])]\n",
    "    \n",
    "# grad_2_re = g.gradient(grads_1_re[0], re)\n",
    "# grad_2_A = g.gradient(grad_1_A, A)\n",
    "\n",
    "# # print(grad_2_re)\n",
    "# # print(grad_2)\n",
    "\n",
    "# ### Torch\n",
    "# re_tc, w1_tc, w_up_tc, b_up_tc, w_down_tc, b_down_tc, W_tc = \\\n",
    "# convert_to_torch([re, w1, w_up, b_up, w_down, b_down, W])\n",
    "# Sigma_up_tc, Pi_up_tc, Sigma_down_tc, Pi_down_tc = \\\n",
    "# convert_to_torch([Sigma_up, Pi_up, Sigma_down, Pi_down])\n",
    "# r_atoms_tc = convert_to_torch([r_atoms])[0]\n",
    "\n",
    "\n",
    "# A_tc, B_tc = fermi_tc(r_atoms_tc, re_tc, w1_tc, w_up_tc, b_up_tc, Sigma_up_tc, Pi_up_tc, \\\n",
    "#               w_down_tc, b_down_tc, Sigma_down_tc, Pi_down_tc)\n",
    "# log_psi_tc = fn(A_tc, B_tc, W_tc)\n",
    "\n",
    "# grad_1_Atc = tcgrad(log_psi_tc.sum(), A_tc, retain_graph=True, create_graph=True)[0]\n",
    "# grad_2_Atc = tcgrad(grad_1_Atc.sum(), A_tc, retain_graph=True)[0]\n",
    "# grad_1_Btc = tcgrad(log_psi_tc.sum(), B_tc, retain_graph=True)[0]\n",
    "# grad_1_Wtc = tcgrad(log_psi_tc.sum(), W_tc, retain_graph=True)[0]\n",
    "# grad_1_retc = tcgrad(log_psi_tc.sum(), re_tc, retain_graph=True, create_graph=True)[0]\n",
    "# grads_1_retc = grad_1_retc.view(-1,3*n_e)\n",
    "# grad_2_retc = tcgrad(grads_1_retc[:,0].sum(), re_tc, retain_graph=True)[0]\n",
    "\n",
    "# grad_1 = tcgrad(log_psi_tc.sum(), A_tc)\n",
    "\n",
    "# validate_log_psi = compare_tensors(log_psi_tc, log_psi)\n",
    "# print(validate_log_psi)\n",
    "# validate_gradW = compare_tensors(grad_1_Wtc, grad_1_W)\n",
    "# print(validate_gradW)\n",
    "\n",
    "# print(compare_tensors(grad_1_Atc, grad_1_A))\n",
    "# print(compare_tensors(grad_1_Btc, grad_1_B))\n",
    "\n",
    "# validate_re = ct(grad_1_retc, grad_1_re)\n",
    "# print('re grad: ', validate_re)\n",
    "\n",
    "# validate_grads_re = ct(grads_1_retc[:,0], grads_1_re[0])\n",
    "# print('re grads: ', validate_grads_re)\n",
    "\n",
    "# validate_2_re = ct(grad_2_retc, grad_2_re)\n",
    "# print('re grad: ', validate_2_re)\n",
    "\n",
    "# validate_2_A = ct(grad_2_Atc, grad_2_A)\n",
    "# print('re grad: ', validate_2_A)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
