{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch as tc\n",
    "import numpy as np\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from torch.autograd import grad as tcgrad\n",
    "DTYPE = tf.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch(tensors):\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        new_tensor = tc.tensor(tensor.numpy(), requires_grad=True)\n",
    "        new_tensors.append(new_tensor)\n",
    "    return new_tensors\n",
    "\n",
    "def fn(A, B, W):\n",
    "    return tc.log(tc.abs(tc.sum(W * A.det().view(-1,n_k,1,1) * B.det().view(-1,n_k,1,1), axis=1, keepdim=True)))\n",
    "\n",
    "def compute_ae_vectors(r_atoms, r_electrons):\n",
    "    # ae_vectors (n_samples, n_electrons, n_atoms, 3)\n",
    "    r_atoms = tf.expand_dims(r_atoms, 1)\n",
    "    r_electrons = tf.expand_dims(r_electrons, 2)\n",
    "    ae_vectors = r_electrons - r_atoms\n",
    "    return ae_vectors\n",
    "\n",
    "def compute_relative_vectors(v1, v2):\n",
    "    relative_vectors = v1.unsqueeze(2) - v2.unsqueeze(1)\n",
    "    return relative_vectors\n",
    "\n",
    "def compare_tensors(ptorch, tflow):\n",
    "    ptorch = ptorch.detach().numpy()\n",
    "    tflow = tflow.numpy()\n",
    "    mask = np.isclose(ptorch, tflow, rtol=0.0, atol=1e-4).astype(np.float32)\n",
    "    return np.mean(mask)\n",
    "\n",
    "def compare_tf_tensors(tflow1, tflow2,rtol=1e-05, atol=1e-08): #\n",
    "    tflow1 = tflow1.numpy()\n",
    "    tflow2 = tflow2.numpy()\n",
    "    mask = np.isclose(tflow1, tflow2).astype(np.float32)\n",
    "    return np.mean(mask) \n",
    "\n",
    "ct = compare_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slogdet_keepdim(tensor):\n",
    "    sign, tensor_out = tf.linalg.slogdet(tensor)\n",
    "    tensor_out = tf.reshape(tensor_out, (*tensor_out.shape, 1, 1))\n",
    "    sign = tf.reshape(sign, (*sign.shape, 1, 1))\n",
    "    return sign, tensor_out\n",
    "\n",
    "\n",
    "def generate_gamma(s):\n",
    "    n_egvs = s.shape[2]\n",
    "    gamma = [tf.reduce_prod(s[:, :, :i], axis=-1) * tf.reduce_prod(s[:, :, i+1:], axis=-1) for i in range(n_egvs-1)]\n",
    "    gamma.append(tf.reduce_prod(s[:, :, :-1], axis=-1))\n",
    "    gamma = tf.stack(gamma, axis=2)\n",
    "    gamma = tf.expand_dims(gamma, axis=2)\n",
    "    return gamma\n",
    "\n",
    "\n",
    "def first_derivative_det(A):\n",
    "    with tf.device(\"/cpu:0\"):  # this is incredible stupid /// its actually not\n",
    "        s, u, v = tf.linalg.svd(A, full_matrices=False)\n",
    "    # s, u, v = tf.linalg.svd(A, full_matrices=False)\n",
    "    v_t = tf.linalg.matrix_transpose(v)\n",
    "    gamma = generate_gamma(s)\n",
    "    sign = (tf.linalg.det(u) * tf.linalg.det(v))[..., None, None]\n",
    "    out = sign * ((u * gamma) @ v_t)\n",
    "    return out, (s, u, v_t, sign)\n",
    "\n",
    "\n",
    "def generate_p(s):\n",
    "    \"\"\"\n",
    "    :param s:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_samples, n_k, n_dim = s.shape\n",
    "    new_shape = (1, 1, 1, n_dim, n_dim)\n",
    "    s = s[..., None, None]\n",
    "    s = tf.tile(s, new_shape)\n",
    "    mask = np.ones(s.shape, dtype=np.bool)\n",
    "    for i in range(n_dim):\n",
    "        for j in range(n_dim):\n",
    "            mask[..., i, i, j] = False\n",
    "            mask[..., j, i, j] = False\n",
    "    mask = tf.convert_to_tensor(mask)\n",
    "    s = tf.where(mask, s, tf.ones_like(s, dtype=s.dtype))\n",
    "    s_prod = tf.reduce_prod(s, axis=-3)\n",
    "    s_prod = tf.linalg.set_diag(s_prod, tf.zeros((s_prod.shape[:-1]), dtype=s.dtype))\n",
    "    return s_prod\n",
    "\n",
    "\n",
    "def second_derivative_det(A, C_dash, *A_cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param A:\n",
    "    :param C_dash:\n",
    "    :param A_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # This function computes the second order derivative of detA wrt to A\n",
    "    # A matrix\n",
    "    # C_bar backward sensitivity\n",
    "    # A_cache cached values returned by grad_det(A)\n",
    "    s, u, v_t, sign = A_cache  # decompose the cache\n",
    "\n",
    "    M = v_t @ tf.linalg.matrix_transpose(C_dash) @ u\n",
    "\n",
    "    p = generate_p(s)\n",
    "\n",
    "    sgn = tf.math.sign(sign)\n",
    "\n",
    "    m_jj = tf.linalg.diag_part(M)\n",
    "    xi = -M * p\n",
    "\n",
    "    xi_diag = p @ tf.expand_dims(m_jj, -1)\n",
    "    xi = tf.linalg.set_diag(xi, tf.squeeze(xi_diag, -1))\n",
    "    return sgn * u @ xi @ v_t\n",
    "\n",
    "\n",
    "def k_sum(x):\n",
    "    return tf.reduce_sum(x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def matrix_sum(x):\n",
    "    return tf.reduce_sum(x, axis=[-2, -1], keepdims=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _log_abs_sum_det_fwd(a, b, w):\n",
    "    a = tf.stop_gradient(a)\n",
    "    b = tf.stop_gradient(b)\n",
    "    w = tf.stop_gradient(w)\n",
    "    \"\"\"\n",
    "\n",
    "    :param a:\n",
    "    :param b:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Take the slogdet of all k determinants\n",
    "    sign_a, logdet_a = slogdet_keepdim(a)\n",
    "    sign_b, logdet_b = slogdet_keepdim(b)\n",
    "\n",
    "    x = logdet_a + logdet_b\n",
    "    xmax = tf.math.reduce_max(x, axis=1, keepdims=True)\n",
    "\n",
    "    unshifted_exp = sign_a * sign_b * tf.exp(x)\n",
    "    unshifted_exp_w = w * unshifted_exp\n",
    "    sign_unshifted_sum = tf.math.sign(tf.reduce_sum(unshifted_exp_w, axis=1, keepdims=True))\n",
    "\n",
    "    exponent = x - xmax\n",
    "    shifted_exp = sign_a * sign_b * tf.exp(exponent)\n",
    "\n",
    "    u = w * shifted_exp\n",
    "    u_sum = tf.reduce_sum(u, axis=1, keepdims=True)\n",
    "    sign_shifted_sum = tf.math.sign(u_sum)\n",
    "    log_psi = tf.math.log(tf.math.abs(u_sum)) + xmax\n",
    "\n",
    "    # Both of these derivations appear to be valid\n",
    "    # activations = shifted_exp\n",
    "    # sensitivities = sign_unshifted_sum * tf.exp(-log_psi)\n",
    "    # dw = sign_unshifted_sum * sign_a * sign_b * tf.exp(x-log_psi)\n",
    "    #\n",
    "    # return log_psi, sign_shifted_sum, activations, sensitivities, \\\n",
    "    #           (a, b, w, unshifted_exp, sign_unshifted_sum, dw, sign_a, logdet_a, sign_b, logdet_b, log_psi)\n",
    "\n",
    "    sensitivities = tf.exp(-log_psi) * sign_unshifted_sum\n",
    "    # sensitivities = tf.exp(xmax-log_psi) * sign_shifted_sum\n",
    "\n",
    "    dw = sign_unshifted_sum * sign_a * sign_b * tf.exp(x - log_psi)\n",
    "\n",
    "    return log_psi, sign_unshifted_sum, unshifted_exp, sensitivities, \\\n",
    "           (a, b, w, unshifted_exp, sign_unshifted_sum, dw, sign_a, logdet_a, sign_b, logdet_b, log_psi)\n",
    "\n",
    "# @tf.function\n",
    "def _log_abs_sum_det_first_order(*fwd_cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param fwd_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a, b, w, unshifted_exp, sign_unshifted_sum, dw, sign_a, logdet_a, sign_b, logdet_b, log_psi = fwd_cache\n",
    "\n",
    "    ddeta, ddeta_cache = first_derivative_det(a)\n",
    "    ddetb, ddetb_cache = first_derivative_det(b)\n",
    "\n",
    "    dfddeta = w * sign_unshifted_sum * sign_b * tf.exp(logdet_b - log_psi)\n",
    "    dfddetb = w * sign_unshifted_sum * sign_a * tf.exp(logdet_a - log_psi)\n",
    "\n",
    "    da = dfddeta * ddeta\n",
    "    db = dfddetb * ddetb\n",
    "\n",
    "    return (da, db, dw), (sign_unshifted_sum, ddeta, ddeta_cache, ddetb, ddetb_cache, dfddeta, dfddetb, da, db, dw)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def _log_abs_sum_det_second_order(a_dash, b_dash, w_dash, *cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a_dash:\n",
    "    :param b_dash:\n",
    "    :param w_dash:\n",
    "    :param cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a, b, w, unshifted_exp, sign_unshifted_sum, _, sign_a, logdet_a, sign_b, logdet_b, log_psi, \\\n",
    "    sign_u, ddeta, ddeta_cache, ddetb, ddetb_cache, dfddeta, dfddetb, da, db, dw = cache\n",
    "\n",
    "    dfddeta_w = dfddeta / w\n",
    "    dfddetb_w = dfddetb / w\n",
    "\n",
    "    ddeta_sum = matrix_sum(a_dash * ddeta)\n",
    "    da_sum = matrix_sum(da * a_dash)\n",
    "    ddetb_sum = matrix_sum(b_dash * ddetb)\n",
    "    db_sum = matrix_sum(db * b_dash)\n",
    "    a_sum = k_sum(dfddeta * ddeta_sum)\n",
    "    b_sum = k_sum(dfddetb * ddetb_sum)\n",
    "\n",
    "    # Compute second deriviate of f wrt to w\n",
    "    d2w = w_dash * -dw * k_sum(dw)\n",
    "\n",
    "    # compute deriviate of df/da wrt to w\n",
    "    dadw = -dw * k_sum(da_sum)\n",
    "    dadw += dfddeta_w * ddeta_sum  # i=j\n",
    "\n",
    "    # compute derivative of df/db wrt to w\n",
    "    dbdw = -dw * k_sum(db_sum)\n",
    "    dbdw += dfddetb_w * ddetb_sum  # i=j\n",
    "\n",
    "    # Compute second derivative of f wrt to a\n",
    "    d2a = -da * a_sum\n",
    "    d2a += dfddeta * second_derivative_det(a, a_dash, *ddeta_cache)  # i=j\n",
    "    # Compute derivative of df/db wrt to a\n",
    "    dbda = -da * b_sum\n",
    "    dbda += ddeta * sign_u * tf.exp(-log_psi) * w * ddetb_sum  # i=j\n",
    "    # Compute derivative of df/dw wrt to a\n",
    "    dwda = w_dash * -da * k_sum(dw)\n",
    "    dwda += w_dash * da / w  # i=j\n",
    "\n",
    "    # Compute second derivative of f wrt to b\n",
    "    d2b = -db * b_sum\n",
    "    d2b += dfddetb * second_derivative_det(b, b_dash, *ddetb_cache)  # i=j\n",
    "    # Compute derivative of df/da wrt to b\n",
    "    dadb = -db * a_sum\n",
    "    dadb += ddetb * sign_u * tf.exp(-log_psi) * w * ddeta_sum  # i=j\n",
    "    # Compute derivative of df/dw wrt to b\n",
    "    dwdb = w_dash * -db * k_sum(dw)\n",
    "    dwdb += w_dash * db / w  # i=j\n",
    "\n",
    "    return (d2a + dbda + dwda), (d2b + dadb + dwdb), \\\n",
    "           (d2w + dadw + dbdw), \\\n",
    "           None, None, None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "# @tf.autograph.experimental.do_not_convert\n",
    "# @tf.function\n",
    "@tf.custom_gradient\n",
    "def first_order_gradient(a_unused, b_unused, w_unused, *fwd_cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a_unused:\n",
    "    :param b_unused:\n",
    "    :param w_unused:\n",
    "    :param fwd_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    (da, db, dw), first_order_cache = _log_abs_sum_det_first_order(*fwd_cache)\n",
    "    return (da, db, dw), \\\n",
    "           lambda a_dash, b_dash, w_dash: _log_abs_sum_det_second_order(\n",
    "               a_dash, b_dash, w_dash, *fwd_cache, *first_order_cache)\n",
    "\n",
    "\n",
    "# @tf.autograph.experimental.do_not_convert\n",
    "# @tf.function\n",
    "@tf.custom_gradient\n",
    "def log_abs_sum_det(a, b, w):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a:\n",
    "    :param b:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    log_psi, sign, act, sens, fwd_cache = _log_abs_sum_det_fwd(a, b, w)\n",
    "\n",
    "    def _first_order_grad(dy, dsg, _, __):\n",
    "        da, db, dw = first_order_gradient(a, b, w, *fwd_cache)\n",
    "\n",
    "        # print('dy', dy)\n",
    "        return dy * da, dy * db, tf.reduce_sum(dy * dw, axis=0, keepdims=True)\n",
    "    # tf.reduce_sum(dy * dw, axis=0, keepdims=True)\n",
    "\n",
    "    return (log_psi, sign, act, sens), _first_order_grad\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('custom_gradient function expected to return', 14, 'gradients but returned', 15, 'instead.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fd766e329201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mgrad_1_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_psi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mgrad_1_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_psi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mgrad_2_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_1_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mlog_psi_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_tc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_tc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_tc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aqua/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aqua/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aqua/lib/python3.7/site-packages/tensorflow_core/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36mactual_grad_fn\u001b[0;34m(*result_grads)\u001b[0m\n\u001b[1;32m    349\u001b[0m       raise ValueError(\n\u001b[1;32m    350\u001b[0m           \u001b[0;34m\"custom_gradient function expected to return\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m           \"gradients but returned\", len(flat_grads), \"instead.\")\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariable_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('custom_gradient function expected to return', 14, 'gradients but returned', 15, 'instead.')"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "n_k = 10\n",
    "ndim = 4\n",
    "A = tf.random.normal((n_samples, n_k, ndim, ndim), dtype=DTYPE)\n",
    "B = tf.random.normal((n_samples, n_k, ndim, ndim), dtype=DTYPE)\n",
    "W = tf.random.normal((1,n_k,1,1), dtype=DTYPE)\n",
    "\n",
    "A_tc, B_tc, W_tc = convert_to_torch([A, B, W])\n",
    "\n",
    "with tf.GradientTape(True) as g:\n",
    "    g.watch(A)\n",
    "    g.watch(W)\n",
    "    g.watch(B)\n",
    "    with tf.GradientTape(True) as gg:\n",
    "        gg.watch(A)\n",
    "        gg.watch(B)\n",
    "        gg.watch(W)\n",
    "        log_psi,_, _, _ = log_abs_sum_det(A, B, W)\n",
    "    grad_1_A = gg.gradient(log_psi, A)\n",
    "    grad_1_B = gg.gradient(log_psi, B)\n",
    "    grad_1_W = gg.gradient(log_psi, W)\n",
    "grad_2_A = g.gradient(grad_1_A, A)\n",
    "\n",
    "log_psi_tc = fn(A_tc, B_tc, W_tc)\n",
    "\n",
    "# first order\n",
    "grad_1_Atc = tcgrad(log_psi_tc.sum(), A_tc, retain_graph=True, create_graph=True)[0]\n",
    "grad_1_Btc = tcgrad(log_psi_tc.sum(), B_tc, retain_graph=True, create_graph=True)[0]\n",
    "grad_1_Wtc = tcgrad(log_psi_tc.sum(), W_tc, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "# second order\n",
    "grad_2_Atc = tcgrad(grad_1_Atc.sum(), A_tc)[0]\n",
    "\n",
    "# validation\n",
    "validate_log_psi = compare_tensors(log_psi_tc, log_psi)\n",
    "print(validate_log_psi)\n",
    "validate_gradW = compare_tensors(grad_1_Wtc, grad_1_W)\n",
    "print(validate_gradW)\n",
    "\n",
    "validate_grad_1_A = compare_tensors(grad_1_Atc, grad_1_A) \n",
    "print(validate_grad_1_A)\n",
    "\n",
    "validate_grad_2_A = compare_tensors(grad_2_Atc, grad_2_A)\n",
    "print(validate_grad_2_A)\n",
    "\n",
    "# if validate_grad1 < 1:\n",
    "#     log_psi_tc = log_psi_tc.reshape(-1)\n",
    "#     log_psi = tf.reshape(log_psi, (-1,))\n",
    "#     for el1, el2 in zip(log_psi_tc, log_psi):\n",
    "#         print(abs(el1.detach().numpy()-el2.numpy()))\n",
    "#     print(log_psi_tc, log_psi)\n",
    "    \n",
    "#     print('determinants: ', A_tc.det(), B_tc.det())\n",
    "# print(compare_tensors(grad_1_tc, grad_1))\n",
    "\n",
    "# print('B: ', ct(B_tc, B))\n",
    "# print('ae_vectors:', compare_tensors(ae_vectors_tc, ae_vectors))\n",
    "# print('exponential: ', compare_tensors(expo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_single_in = 4 * n_atoms\n",
    "nf_single_hidden = 128\n",
    "\n",
    "nf_hidden_pairwise = \n",
    "\n",
    "# params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class fermiNet(tk.Model):\n",
    "    \"\"\"\n",
    "    fermi net, baby\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 r_atoms,\n",
    "                 n_electrons,\n",
    "                 n_atoms,\n",
    "                 n_spin_up,\n",
    "                 n_spin_down,\n",
    "                 nf_hidden_single,\n",
    "                 nf_hidden_pairwise,\n",
    "                 n_determinants,\n",
    "                 env_init,\n",
    "                 full_pairwise,\n",
    "                 mix_final,\n",
    "                 mix_input):\n",
    "\n",
    "        super(fermiNet, self).__init__()\n",
    "\n",
    "        self.mix_input = mix_input\n",
    "        self.mix_final = mix_final\n",
    "        self.full_pairwise = full_pairwise\n",
    "\n",
    "        # --- initializations\n",
    "        if full_pairwise:\n",
    "            n_pairwise = n_electrons ** 2  # - n_electrons\n",
    "        else:\n",
    "            n_pairwise = n_electrons ** 2 - n_electrons\n",
    "\n",
    "        nf_single_in = 4 * n_atoms\n",
    "        nf_pairwise_in = 4\n",
    "        nf_single_in_mixed = 3 * nf_single_in + 2 * nf_pairwise_in\n",
    "        nf_single_intermediate_in = nf_hidden_single * 3 + nf_hidden_pairwise * 2\n",
    "        nf_pairwise_intermediate_in = nf_hidden_pairwise\n",
    "\n",
    "        # --- use internally\n",
    "        self.n_electrons = n_electrons\n",
    "        self.n_atoms = n_atoms\n",
    "        self.n_spin_up = n_spin_up\n",
    "        self.n_spin_down = n_electrons - n_spin_up\n",
    "        n_spins = n_spin_up + n_spin_down\n",
    "        self.n_determinants = n_determinants\n",
    "        self.r_atoms = r_atoms\n",
    "        self.n_pairwise = n_pairwise\n",
    "\n",
    "        # --- model\n",
    "        if self.mix_input:\n",
    "            self.input_mixer = Mixer(n_electrons, nf_single_in, n_pairwise, nf_pairwise_in, n_spin_up, n_spin_down, full_pairwise, input_mixer=True)\n",
    "            nf_single_in = nf_single_in_mixed\n",
    "\n",
    "        self.single_stream_in = Stream(nf_single_in, nf_hidden_single, n_spins, gpu_id, 0)\n",
    "        self.pairwise_stream_in = Stream(nf_pairwise_in, nf_hidden_pairwise, n_pairwise, gpu_id, 0)\n",
    "        self.mixer_in = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "\n",
    "        self.s1 = Stream(nf_single_intermediate_in, nf_hidden_single, n_spins, gpu_id, 1)\n",
    "        self.p1 = Stream(nf_pairwise_intermediate_in, nf_hidden_pairwise, n_pairwise, gpu_id, 1)\n",
    "        self.m1 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "\n",
    "        self.s2 = Stream(nf_single_intermediate_in, nf_hidden_single, n_spins, gpu_id, 2)\n",
    "        self.p2 = Stream(nf_pairwise_intermediate_in, nf_hidden_pairwise, n_pairwise, gpu_id, 2)\n",
    "        self.m2 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "\n",
    "        if self.mix_final:\n",
    "            self.s3 = Stream(nf_single_intermediate_in, nf_hidden_single, n_spins, gpu_id, 3)\n",
    "            self.p3 = Stream(nf_pairwise_intermediate_in, nf_hidden_pairwise, n_pairwise, gpu_id, 3)\n",
    "            self.m3 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "            n_envelope_in = nf_single_intermediate_in\n",
    "        else:\n",
    "            self.final_single_stream = Stream(nf_single_intermediate_in, nf_hidden_single, n_spins, gpu_id, 3)\n",
    "            n_envelope_in = nf_hidden_single\n",
    "\n",
    "        self.envelopes = \\\n",
    "            envelopesLayer(n_spin_up, n_spin_down, n_atoms, n_envelope_in, n_determinants, env_init, gpu_id)\n",
    "\n",
    "        # self.output_layer = tf.Variable(initializer(n_determinants, (1,n_determinants,1,1), 1, _))\n",
    "        # self.output_layer = tf.Variable(tf.ones((1, n_determinants, 1, 1))/n_determinants, name='w_1')\n",
    "        self.output_layer = tf.Variable(env_initializer(16, (1, n_determinants, 1, 1), 1, env_init / n_determinants),\n",
    "                                        name='wf_1')\n",
    "        # self.epoch = 1\n",
    "\n",
    "    # @tf.function  # phase = 0, 1, 2 // test, supervised, unsupervised\n",
    "    def call(self, r_electrons):\n",
    "        n_samples = r_electrons.shape[0]\n",
    "        r_atoms = tf.tile(self.r_atoms, (n_samples, 1, 1))\n",
    "\n",
    "        # --- computing inputs\n",
    "        # single_inputs: (b, n_electrons, 4), pairwise_inputs: (, n_pairwise, 4)\n",
    "        ae_vectors = compute_ae_vectors(r_atoms, r_electrons)\n",
    "        single, pairwise = compute_inputs(r_electrons, n_samples, ae_vectors, self.n_atoms, self.n_electrons, self.full_pairwise)\n",
    "\n",
    "        if self.mix_input:\n",
    "            single = self.input_mixer(single, pairwise, n_samples, self.n_electrons)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # --- input layer\n",
    "        single, a_in_s, s_in_s = self.single_stream_in(single, n_samples, self.n_electrons)\n",
    "        pairwise, a_in_p, s_in_p = self.pairwise_stream_in(pairwise, n_samples, self.n_pairwise)\n",
    "        single_mix = self.mixer_in(single, pairwise, n_samples, self.n_electrons)\n",
    "\n",
    "        # --- intermediate layers\n",
    "        tmp, a_1_s, s_1_s = self.s1(single_mix, n_samples, self.n_electrons)\n",
    "        single += tmp\n",
    "        tmp, a_1_p, s_1_p = self.p1(pairwise, n_samples, self.n_pairwise)\n",
    "        pairwise += tmp\n",
    "        single_mix = self.m1(single, pairwise, n_samples, self.n_electrons)\n",
    "\n",
    "        tmp, a_2_s, s_2_s = self.s2(single_mix, n_samples, self.n_electrons)\n",
    "        single += tmp\n",
    "        tmp, a_2_p, s_2_p = self.p2(pairwise, n_samples, self.n_pairwise)\n",
    "        pairwise += tmp\n",
    "        single_mix = self.m2(single, pairwise, n_samples, self.n_electrons)\n",
    "\n",
    "        # --- final layer\n",
    "        if self.mix_final:\n",
    "            tmp, a_3_s, s_3_s = self.s3(single_mix, n_samples, self.n_electrons)\n",
    "            single += tmp\n",
    "            tmp, a_3_p, s_3_p = self.p3(pairwise, n_samples, self.n_pairwise)\n",
    "            pairwise += tmp\n",
    "            single = self.m3(single, pairwise, n_samples, self.n_electrons)\n",
    "        else:\n",
    "            single, a_f, s_f = self.final_single_stream(single_mix, n_samples, self.n_electrons)\n",
    "            # single += tmp remember to include back in if issues\n",
    "\n",
    "\n",
    "        # --- envelopes\n",
    "        spin_up_determinants, spin_down_determinants, a_up, s_up, a_down, s_down = \\\n",
    "            self.envelopes(single, ae_vectors, n_samples)\n",
    "\n",
    "        # --- logabsdet\n",
    "        log_psi, sign, a, s = log_abs_sum_det(spin_up_determinants, spin_down_determinants, self.output_layer)\n",
    "\n",
    "        # yep # 7, 8, 9, 10, 11, 12 #\n",
    "        if self.mix_final:\n",
    "            activation = (a_in_s, a_in_p, a_1_s, a_1_p, a_2_s, a_2_p, a_3_s, a_3_p,\n",
    "                          a_up[0], a_up[1], a_up[2], a_down[0], a_down[1], a_down[2], a)\n",
    "            sensitivity = (s_in_s, s_in_p, s_1_s, s_1_p, s_2_s, s_2_p, s_3_s, s_3_p,\n",
    "                           s_up[0], s_up[1], s_up[2], s_down[0], s_down[1], s_down[2], s)\n",
    "        else:\n",
    "            activation = (a_in_s, a_in_p, a_1_s, a_1_p, a_2_s, a_2_p, a_f,\n",
    "                          a_up[0], a_up[1], a_up[2], a_down[0], a_down[1], a_down[2], a)\n",
    "            sensitivity = (s_in_s, s_in_p, s_1_s, s_1_p, s_2_s, s_2_p, s_f,\n",
    "                           s_up[0], s_up[1], s_up[2], s_down[0], s_down[1], s_down[2], s)\n",
    "\n",
    "        return tf.squeeze(log_psi), sign, activation, sensitivity, spin_up_determinants, spin_down_determinants\n",
    "\n",
    "\n",
    "\n",
    "class envelopesLayer(tk.Model):\n",
    "    def __init__(self, n_spin_up, n_spin_down, n_atoms, nf_single, n_determinants, env_init, gpu_id):\n",
    "        super(envelopesLayer, self).__init__()\n",
    "        # --- variables\n",
    "        self.n_spin_up = n_spin_up\n",
    "        self.n_spin_down = n_spin_down\n",
    "        self.n_k = n_determinants\n",
    "        self.n_atoms = n_atoms\n",
    "\n",
    "        # --- envelopes\n",
    "        self.spin_up_envelope = envelopeLayer(n_spin_up, n_atoms, nf_single, n_determinants, env_init, gpu_id, name='up')\n",
    "        self.spin_down_envelope = envelopeLayer(n_spin_down, n_atoms, nf_single, n_determinants, env_init, gpu_id, name='down')\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, inputs, ae_vectors, n_samples):\n",
    "        # --- arrange inputs\n",
    "        spin_up_ae_vectors, spin_down_ae_vectors = tf.split(ae_vectors, [self.n_spin_up, self.n_spin_down], axis=1)\n",
    "        spin_up_inputs, spin_down_inputs = tf.split(inputs, [self.n_spin_up, self.n_spin_down], axis=1)\n",
    "\n",
    "        # --- envelopes\n",
    "        spin_up_output, a_up, s_up = self.spin_up_envelope(spin_up_inputs, spin_up_ae_vectors,\n",
    "                                               n_samples, self.n_spin_up, self.n_k, self.n_atoms)\n",
    "        spin_down_output, a_down, s_down = self.spin_down_envelope(spin_down_inputs, spin_down_ae_vectors,\n",
    "                                                   n_samples, self.n_spin_down, self.n_k, self.n_atoms)\n",
    "\n",
    "        return spin_up_output, spin_down_output, a_up, s_up, a_down, s_down\n",
    "\n",
    "\n",
    "class envelopeLayer(tk.Model):\n",
    "    def __init__(self, n_spins, n_atoms, nf_single, n_determinants, env_init, gpu_id, name=''):\n",
    "        super(envelopeLayer, self).__init__()\n",
    "        # k: n_determinants, i: n_electrons, f: n_features\n",
    "        w = initializer(nf_single, (n_determinants, n_spins, nf_single, 1), 1, None)\n",
    "        b = tf.zeros((n_determinants, n_spins, 1, 1))\n",
    "        w = tf.concat((w, b), axis=2)\n",
    "\n",
    "        self.w = tf.Variable(w, name='env_%s_w_%i' % (name, n_spins))\n",
    "\n",
    "        self.Sigma = tf.Variable(env_initializer(3, (n_determinants, n_spins, n_atoms, 3, 3), 3, env_init),\n",
    "                                 name='env_%s_sigma_%i' % (name, n_spins))\n",
    "\n",
    "        self.Pi = tf.Variable(env_initializer(n_atoms, (n_determinants, n_spins, n_atoms, 1), 1, env_init),\n",
    "                              name='env_%s_pi_%i' % (name, n_spins))\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, inputs, ae_vectors, n_samples, n_spins, n_k, n_atoms):\n",
    "        # inputs: (n_samples, n_electrons, nf_single)\n",
    "        # ae_vectors: (n_samples, n_electrons, n_atoms, 3)\n",
    "        # n: n_samples, e: n_electrons, f: nf_single, i: n_electrons, k: n_determinants\n",
    "\n",
    "        # env_w 'njf,kifs->nkjis'\n",
    "        inputs_w_bias = tf.concat((inputs, tf.ones((n_samples, n_spins, 1))), axis=-1)\n",
    "        factor = tf.einsum('njf,kifs->njkis', inputs_w_bias, self.w)\n",
    "\n",
    "        # k: n_determinants, i: n_electrons, m: n_atoms, n: n_samples, j: n_electrons\n",
    "        # env_sigma 'njmv,kimvc->nkjimc'\n",
    "        exponent = tf.einsum('njmv,kimvc->njkimc', ae_vectors, self.Sigma)\n",
    "        exponential = tf.exp(-tf.norm(exponent, axis=-1))\n",
    "\n",
    "        # env_pi 'njkim,kims->nkjis'\n",
    "        exp = tf.einsum('njkim,kims->njkis', exponential, self.Pi)\n",
    "\n",
    "        output = factor * exp\n",
    "        output = tf.transpose(output, perm=(0, 2, 3, 1, 4))  # ij ordering doesn't matter / slight numerical diff\n",
    "\n",
    "        return tf.squeeze(output, -1), (inputs, ae_vectors, exponential), (factor, exponent, exp)\n",
    "\n",
    "\n",
    "class Stream(tk.Model):\n",
    "    \"\"\"\n",
    "    single / pairwise electron streams\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, n_spins, gpu_id, node):\n",
    "        super(Stream, self).__init__()\n",
    "        # --- variables\n",
    "        # lim = tf.math.sqrt(6 / (in_dim + out_dim))\n",
    "        # w = tf.concat((tf.random.uniform((in_dim, out_dim), minval=-lim, maxval=lim), tf.zeros((1, out_dim))), axis=0)\n",
    "        w = initializer(in_dim, (in_dim, out_dim), out_dim, None)\n",
    "        b = tf.zeros((1, out_dim))\n",
    "        # b = tf.random.normal((1, out_dim), stddev=std, dtype=dtype)\n",
    "        w = tf.concat((w, b), axis=0)\n",
    "        self.w = tf.Variable(w, name='stream%i_%i' % (node, n_spins))\n",
    "\n",
    "    def call(self, inputs, n_samples, n_streams):\n",
    "        inputs_w_bias = tf.concat((inputs, tf.ones((n_samples, n_streams, 1))), axis=-1)\n",
    "        out1 = inputs_w_bias @ self.w\n",
    "        out2 = tf.nn.tanh(out1)\n",
    "        return out2, inputs, out1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def fermi_tf(r_atoms, re, w1, w_up, b_up, Sigma_up, Pi_up, w_down, b_down, Sigma_down, Pi_down):\n",
    "    ae_vectors = compute_ae_vectors(r_atoms, re)\n",
    "    r2 = tf.einsum('fv,niv->nif',w1,re)\n",
    "\n",
    "    # Envelopes\n",
    "    ae_vectors_up = ae_vectors[:,:n_up,...]\n",
    "    r_up = r2[:,:n_up,:]\n",
    "    factor = tf.einsum('njf,kfi->nkji', r_up, w_up)\n",
    "    factor = factor + b_up\n",
    "    exp = tf.einsum('kimvc,njmv->nkijmc', Sigma_up, ae_vectors_up)\n",
    "    exponential = tf.exp(-tf.norm(exp, axis=-1))\n",
    "    exp = tf.einsum('nkijm,kim->nkij', exponential, Pi_up)\n",
    "    A = factor * exp\n",
    "\n",
    "    ae_vectors_down = ae_vectors[:,n_up:,...]\n",
    "    r_down = r2[:,n_up:,:]\n",
    "    factor = tf.einsum('njf,kfi->nkji', r_down, w_down)\n",
    "    factor = factor + b_down\n",
    "    exp = tf.einsum('kimvc,njmv->nkijmc', Sigma_down, ae_vectors_down)\n",
    "    exponential = tf.exp(-tf.norm(exp, axis=-1))\n",
    "    exp = tf.einsum('nkijm,kim->nkij', exponential, Pi_down)\n",
    "    B = factor * exp\n",
    "    \n",
    "    return A, B\n",
    "    \n",
    "    \n",
    "def fermi_tc(r_atoms_tc, re_tc, w1_tc,  w_up_tc, b_up_tc, Sigma_up_tc, Pi_up_tc, \\\n",
    "              w_down_tc, b_down_tc, Sigma_down_tc, Pi_down_tc):\n",
    "    ae_vectors_tc = compute_relative_vectors(re_tc,r_atoms_tc)\n",
    "    # ### Torch\n",
    "    r2_tc = tc.einsum('fv,niv->nif',[w1_tc,re_tc])\n",
    "\n",
    "    # Envelopes\n",
    "    ae_vectors_up_tc = ae_vectors_tc[:,:n_up,...]\n",
    "    r_up_tc = r2_tc[:,:n_up,:]\n",
    "    factor = tc.einsum('njf,kfi->nkji', [r_up_tc, w_up_tc])\n",
    "    factor = factor + b_up_tc\n",
    "    exp = tc.einsum('kimvc,njmv->nkijmc', [Sigma_up_tc, ae_vectors_up_tc])\n",
    "    exponential = tc.exp(-exp.norm(dim=-1))\n",
    "    exp = tc.einsum('nkijm,kim->nkij', [exponential, Pi_up_tc])\n",
    "    A_tc = factor * exp\n",
    "\n",
    "    ae_vectors_down_tc = ae_vectors_tc[:,n_up:,...]\n",
    "    r_down_tc = r2_tc[:,n_up:,:]\n",
    "    factor = tc.einsum('njf,kfi->nkji', [r_down_tc, w_down_tc])\n",
    "    factor = factor + b_down_tc\n",
    "    exp = tc.einsum('kimvc,njmv->nkijmc', [Sigma_down_tc, ae_vectors_down_tc])\n",
    "    exponential = tc.exp(-exp.norm(dim=-1))\n",
    "    exp = tc.einsum('nkijm,kim->nkij', [exponential, Pi_down_tc])\n",
    "    B_tc = factor * exp\n",
    "    return A_tc, B_tc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "n_k = 10\n",
    "n_f = 20\n",
    "n_up = 2\n",
    "n_e = 4\n",
    "n_down = n_e - n_up\n",
    "n_atom = 1 \n",
    "\n",
    "### Parameters\n",
    "# First\n",
    "r_atoms = tf.zeros((n_samples, n_atom, 3), dtype=DTYPE)\n",
    "re = tf.random.normal((n_samples, n_e, 3), dtype=DTYPE) #* 100.\n",
    "w1 = tf.random.normal((n_f, 3), dtype=DTYPE)\n",
    "# up\n",
    "w_up = tf.random.normal((n_k, n_f, n_up), dtype=DTYPE)\n",
    "b_up = tf.random.normal((n_k, n_up, 1), dtype=DTYPE)\n",
    "Sigma_up = tf.random.normal((n_k, n_up, n_atom, 3, 3), dtype=DTYPE)\n",
    "Pi_up = tf.random.normal((n_k, n_up, n_atom), dtype=DTYPE)\n",
    "# down\n",
    "w_down = tf.random.normal((n_k, n_f, n_down), dtype=DTYPE)\n",
    "b_down = tf.random.normal((n_k, n_down, 1), dtype=DTYPE)\n",
    "Sigma_down = tf.random.normal((n_k, n_down, n_atom, 3, 3), dtype=DTYPE)\n",
    "Pi_down = tf.random.normal((n_k, n_down, n_atom), dtype=DTYPE)\n",
    "# Weights\n",
    "W = tf.random.normal((1, n_k, 1, 1), dtype=DTYPE)\n",
    "\n",
    "with tf.GradientTape(True) as g:\n",
    "    g.watch(W)\n",
    "    g.watch(re)\n",
    "    with tf.GradientTape(True) as gg:\n",
    "        gg.watch(re)\n",
    "        gg.watch(W)\n",
    "        A, B = fermi_tf(r_atoms, re, w1,  w_up, b_up, Sigma_up, Pi_up, \\\n",
    "                         w_down, b_down, Sigma_down, Pi_down)\n",
    "        log_psi,_ = log_abs_sum_det(A, B, W)\n",
    "\n",
    "    grad_1_A = gg.gradient(log_psi, A)\n",
    "    grad_1_B = gg.gradient(log_psi, B)\n",
    "    grad_1_W = gg.gradient(log_psi, W)\n",
    "    grad_1_re = gg.gradient(log_psi, re)\n",
    "    grads_1_re = tf.reshape(grad_1_re, (-1, n_e*3))\n",
    "    grads_1_re = [grads_1_re[..., i] for i in range(grads_1_re.shape[-1])]\n",
    "    \n",
    "grad_2_re = g.gradient(grads_1_re[0], re)\n",
    "grad_2_A = g.gradient(grad_1_A, A)\n",
    "\n",
    "# print(grad_2_re)\n",
    "# print(grad_2)\n",
    "\n",
    "### Torch\n",
    "re_tc, w1_tc, w_up_tc, b_up_tc, w_down_tc, b_down_tc, W_tc = \\\n",
    "convert_to_torch([re, w1, w_up, b_up, w_down, b_down, W])\n",
    "Sigma_up_tc, Pi_up_tc, Sigma_down_tc, Pi_down_tc = \\\n",
    "convert_to_torch([Sigma_up, Pi_up, Sigma_down, Pi_down])\n",
    "r_atoms_tc = convert_to_torch([r_atoms])[0]\n",
    "\n",
    "\n",
    "A_tc, B_tc = fermi_tc(r_atoms_tc, re_tc, w1_tc, w_up_tc, b_up_tc, Sigma_up_tc, Pi_up_tc, \\\n",
    "              w_down_tc, b_down_tc, Sigma_down_tc, Pi_down_tc)\n",
    "log_psi_tc = fn(A_tc, B_tc, W_tc)\n",
    "\n",
    "grad_1_Atc = tcgrad(log_psi_tc.sum(), A_tc, retain_graph=True, create_graph=True)[0]\n",
    "grad_2_Atc = tcgrad(grad_1_Atc.sum(), A_tc, retain_graph=True)[0]\n",
    "grad_1_Btc = tcgrad(log_psi_tc.sum(), B_tc, retain_graph=True)[0]\n",
    "grad_1_Wtc = tcgrad(log_psi_tc.sum(), W_tc, retain_graph=True)[0]\n",
    "grad_1_retc = tcgrad(log_psi_tc.sum(), re_tc, retain_graph=True, create_graph=True)[0]\n",
    "grads_1_retc = grad_1_retc.view(-1,3*n_e)\n",
    "grad_2_retc = tcgrad(grads_1_retc[:,0].sum(), re_tc, retain_graph=True)[0]\n",
    "\n",
    "grad_1 = tcgrad(log_psi_tc.sum(), A_tc)\n",
    "\n",
    "validate_log_psi = compare_tensors(log_psi_tc, log_psi)\n",
    "print(validate_log_psi)\n",
    "validate_gradW = compare_tensors(grad_1_Wtc, grad_1_W)\n",
    "print(validate_gradW)\n",
    "\n",
    "print(compare_tensors(grad_1_Atc, grad_1_A))\n",
    "print(compare_tensors(grad_1_Btc, grad_1_B))\n",
    "\n",
    "validate_re = ct(grad_1_retc, grad_1_re)\n",
    "print('re grad: ', validate_re)\n",
    "\n",
    "validate_grads_re = ct(grads_1_retc[:,0], grads_1_re[0])\n",
    "print('re grads: ', validate_grads_re)\n",
    "\n",
    "validate_2_re = ct(grad_2_retc, grad_2_re)\n",
    "print('re grad: ', validate_2_re)\n",
    "\n",
    "validate_2_A = ct(grad_2_Atc, grad_2_A)\n",
    "print('re grad: ', validate_2_A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_1_retc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_1_re_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_2_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_2_retc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- compare nico to new \n",
    "\n",
    "# --- outputs the same\n",
    "\n",
    "# --- grad1 the same (ABW)\n",
    "\n",
    "# --- grad1 the same (r)\n",
    "\n",
    "# --- grad2 the same (ABW)\n",
    "\n",
    "# --- grad2 the same (r)\n",
    "\n",
    "# --- speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
