{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch as tc\n",
    "import numpy as np\n",
    "import os \n",
    "from tf_utils.utils import *\n",
    "from tc_utils.utils import *\n",
    "import tensorflow.keras as tk\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from torch.autograd import grad as tcgrad\n",
    "npDTYPE = np.float32\n",
    "tcDTYPE = tc.float32\n",
    "tfDTYPE = tf.float32\n",
    "\n",
    "npDTYPE = np.float64\n",
    "tcDTYPE = tc.float64\n",
    "tfDTYPE = tf.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch(tensors):\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        new_tensor = tc.tensor(tensor.numpy(), requires_grad=True)\n",
    "        new_tensors.append(new_tensor)\n",
    "    return new_tensors\n",
    "\n",
    "def tcVariable(tensor):\n",
    "    return tc.autograd.Variable(tc.tensor(tensor))\n",
    "\n",
    "def compute_relative_vectors(v1, v2):\n",
    "    relative_vectors = v1.unsqueeze(2) - v2.unsqueeze(1)\n",
    "    return relative_vectors\n",
    "\n",
    "# def compare_tensors(ptorch, tflow):\n",
    "#     ptorch = ptorch.detach().numpy()\n",
    "#     tflow = tflow.numpy()\n",
    "#     mask = np.isclose(ptorch, tflow, rtol=0.0, atol=1e-4).astype(np.float32)\n",
    "#     return np.mean(mask)\n",
    "\n",
    "def compare_tensors(ptorch, tflow):\n",
    "    ptorch = ptorch.detach().numpy()\n",
    "    tflow = tflow.numpy()\n",
    "    ptorch = np.reshape(ptorch, tflow.shape)\n",
    "    x = np.mean(np.abs(ptorch - tflow))\n",
    "    return x\n",
    "\n",
    "# def compare_tf_tensors(tflow1, tflow2,rtol=1e-05, atol=1e-08): #\n",
    "#     tflow1 = tflow1.numpy()\n",
    "#     tflow2 = tflow2.numpy()\n",
    "#     mask = np.isclose(tflow1, tflow2).astype(np.float32)\n",
    "#     return np.mean(mask) \n",
    "\n",
    "def compare_tf_tensors(tflow1, tflow2,rtol=1e-05, atol=1e-08): #\n",
    "    tflow1 = tflow1.numpy()\n",
    "    tflow2 = tflow2.numpy()\n",
    "    return np.mean(np.abs(tflow1 - tflow2))\n",
    "\n",
    "ct = compare_tensors\n",
    "\n",
    "def tflaplacian(model, r_electrons):\n",
    "    n_electrons = r_electrons.shape[1]\n",
    "    r_electrons = tf.reshape(r_electrons, (-1, n_electrons*3))\n",
    "    r_s = [r_electrons[..., i] for i in range(r_electrons.shape[-1])]\n",
    "    with tf.GradientTape(True) as g:\n",
    "        [g.watch(r) for r in r_s]\n",
    "        r_electrons = tf.stack(r_s, -1)\n",
    "        r_electrons = tf.reshape(r_electrons, (-1, n_electrons, 3))\n",
    "        with tf.GradientTape(True) as gg:\n",
    "            gg.watch(r_electrons)\n",
    "            log_phi = model(r_electrons)\n",
    "        dlogphi_dr = gg.gradient(log_phi, r_electrons)\n",
    "        dlogphi_dr = tf.reshape(dlogphi_dr, (-1, n_electrons*3))\n",
    "        grads = [dlogphi_dr[..., i] for i in range(dlogphi_dr.shape[-1])]\n",
    "    d2logphi_dr2 = tf.stack([g.gradient(grad, r) for grad, r in zip(grads, r_s)], -1)\n",
    "    return dlogphi_dr**2, d2logphi_dr2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fns\n",
    "\n",
    "def linear(w, a, n_samples, n_conv, flow=True):\n",
    "    if flow:\n",
    "        a = tf.concat((a, tf.ones((n_samples, n_conv, 1), dtype=tfDTYPE)), axis=-1)\n",
    "        out = tf.tanh(a @ w)\n",
    "        return out\n",
    "    else:\n",
    "        a = tc.cat((a, tc.ones((n_samples, n_conv, 1), dtype=tcDTYPE)), dim=-1)\n",
    "        out = tc.tanh(a @ w)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def env(inputs, ae_vecs, env_w, env_sigma, env_pi, n_samples, n_spins, flow=True):\n",
    "    if flow:\n",
    "        inputs = tf.concat((inputs, tf.ones((n_samples, n_spins, 1), dtype=tfDTYPE)), axis=-1)\n",
    "        factor = tf.einsum('njf,kifs->njkis', inputs, env_w)\n",
    "        \n",
    "        exponent = tf.einsum('njmv,kimvc->njkimc', ae_vecs, env_sigma)\n",
    "        exponential = tf.exp(-tf.norm(exponent, axis=-1))\n",
    "\n",
    "        exp = tf.einsum('njkim,kims->njkis', exponential, env_pi)\n",
    "\n",
    "        output = factor * exp\n",
    "        output = tf.transpose(output, perm=(0, 2, 3, 1, 4))  # ij ordering doesn't matter / slight numerical diff\n",
    "\n",
    "        return tf.squeeze(output, -1)  \n",
    "        \n",
    "    else:\n",
    "        inputs = tc.cat((inputs, tc.ones((n_samples, n_spins, 1), dtype=tcDTYPE)), dim=-1)\n",
    "        factor = tc.einsum('njf,kifs->njkis', inputs, env_w)\n",
    "        \n",
    "        exponent = tc.einsum('njmv,kimvc->njkimc', ae_vecs, env_sigma)\n",
    "        exponential = tc.exp(-tc.norm(exponent, dim=-1))\n",
    "\n",
    "        exp = tc.einsum('njkim,kims->njkis', exponential, env_pi)\n",
    "\n",
    "        output = factor * exp\n",
    "        output = output.permute((0, 2, 3, 1, 4))  # ij ordering doesn't matter / slight numerical diff\n",
    "\n",
    "        return output.squeeze(-1)  \n",
    "    \n",
    "def compute_inputs(r_electrons, n_samples, ae_vectors, n_atoms, n_electrons, full_pairwise, flow=True):\n",
    "    # r_atoms: (n_atoms, 3)\n",
    "    # r_electrons: (n_samples, n_electrons, 3)\n",
    "    # ae_vectors: (n_samples, n_electrons, n_atoms, 3)\n",
    "    if flow:\n",
    "        ae_distances = tf.norm(ae_vectors, axis=-1, keepdims=True)\n",
    "        single_inputs = tf.concat((ae_vectors, ae_distances), axis=-1)\n",
    "        single_inputs = tf.reshape(single_inputs, (-1, n_electrons, 4*n_atoms))\n",
    "\n",
    "        re1 = tf.expand_dims(r_electrons, 2)\n",
    "        re2 = tf.transpose(re1, perm=(0, 2, 1, 3))\n",
    "        ee_vectors = re1 - re2\n",
    "\n",
    "        # ** full pairwise\n",
    "        if full_pairwise:\n",
    "            # eye_mask = tf.expand_dims(tf.expand_dims(tf.eye(n_electrons, dtype=tf.bool), 0), -1)\n",
    "            # tmp = tf.where(eye_mask, 1., tf.norm(ee_vectors, keepdims=True, axis=-1))\n",
    "            # ee_distances = tf.where(eye_mask, tf.zeros_like(eye_mask, dtype=tf.float32), tmp)\n",
    "            ee_vectors = tf.reshape(ee_vectors, (-1, n_electrons**2, 3))\n",
    "            ee_distances = safe_norm(ee_vectors)\n",
    "            pairwise_inputs = tf.concat((ee_vectors, ee_distances), axis=-1)\n",
    "            # pairwise_inputs = tf.reshape(pairwise_inputs, (-1, n_electrons**2, 4))\n",
    "        else:\n",
    "            # ** partial pairwise\n",
    "            mask = tf.eye(n_electrons, dtype=tf.bool)\n",
    "            mask = ~tf.tile(tf.expand_dims(tf.expand_dims(mask, 0), 3), (n_samples, 1, 1, 3))\n",
    "\n",
    "            ee_vectors = tf.boolean_mask(ee_vectors, mask)\n",
    "            ee_vectors = tf.reshape(ee_vectors, (-1, n_electrons**2 - n_electrons, 3))\n",
    "            ee_distances = tf.norm(ee_vectors, axis=-1, keepdims=True)\n",
    "\n",
    "            pairwise_inputs = tf.concat((ee_vectors, ee_distances), axis=-1)\n",
    "\n",
    "        return single_inputs, pairwise_inputs\n",
    "    else:\n",
    "        ae_distances = tc.norm(ae_vectors, dim=-1, keepdim=True)\n",
    "        single_inputs = tc.cat((ae_vectors, ae_distances), dim=-1)\n",
    "        single_inputs = single_inputs.view((-1, n_electrons, 4 * n_atoms))\n",
    "\n",
    "        re1 = r_electrons.unsqueeze(2)\n",
    "        re2 = re1.permute((0, 2, 1, 3))\n",
    "        ee_vectors = re1 - re2\n",
    "\n",
    "        # ** full pairwise\n",
    "        if full_pairwise:\n",
    "            # eye_mask = tf.expand_dims(tf.expand_dims(tf.eye(n_electrons, dtype=tf.bool), 0), -1)\n",
    "            # tmp = tf.where(eye_mask, 1., tf.norm(ee_vectors, keepdims=True, axis=-1))\n",
    "            # ee_distances = tf.where(eye_mask, tf.zeros_like(eye_mask, dtype=tf.float32), tmp)\n",
    "            ee_vectors = ee_vectors.view((-1, n_electrons ** 2, 3))\n",
    "            ee_distances = tc.norm(ee_vectors, dim=-1, keepdim=True) + 1e-16\n",
    "            pairwise_inputs = tc.cat((ee_vectors, ee_distances), dim=-1)\n",
    "            # pairwise_inputs = tf.reshape(pairwise_inputs, (-1, n_electrons**2, 4))\n",
    "        else:\n",
    "            print('NOT IMPLEMENTED')\n",
    "\n",
    "        return single_inputs, pairwise_inputs\n",
    "    \n",
    "def compute_ae_vectors(r_atoms, r_electrons, flow):\n",
    "    # ae_vectors (n_samples, n_electrons, n_atoms, 3)\n",
    "    if flow:\n",
    "        r_atoms = tf.expand_dims(r_atoms, 1)\n",
    "        r_electrons = tf.expand_dims(r_electrons, 2)\n",
    "        ae_vectors = r_electrons - r_atoms\n",
    "    else:\n",
    "        r_atoms = r_atoms.unsqueeze(1)\n",
    "        r_electrons = r_electrons.unsqueeze(2)\n",
    "        ae_vectors = r_electrons - r_atoms\n",
    "    return ae_vectors\n",
    "\n",
    "\n",
    "def init(in_dim, weight_shape, out_dim, env_init=0.0):\n",
    "    if env_init == 0.0:\n",
    "        minval = np.maximum(-1., -(6/(in_dim+out_dim))**0.5)\n",
    "        maxval = np.minimum(1., (6/(in_dim+out_dim))**0.5)\n",
    "        weights = np.random.uniform(size=weight_shape, low=minval, high=maxval)\n",
    "    else:\n",
    "        weights = np.random.uniform(size=weight_shape, low=-env_init, high=env_init)\n",
    "    return weights.astype(npDTYPE)\n",
    "\n",
    "def apbi(w, axis=None):\n",
    "    if axis is None:\n",
    "        shape = (1, *w.shape[1:])\n",
    "        b = np.ones(shape).astype(npDTYPE)\n",
    "        return np.concatenate((w, b), axis=0)\n",
    "    shape = (*w.shape[:axis], 1, *w.shape[axis+1:])\n",
    "    b = np.ones(shape).astype(npDTYPE)\n",
    "    return np.concatenate((w, b), axis=axis)\n",
    "\n",
    "\n",
    "# model architecture\n",
    "full_pairwise = True\n",
    "n_atoms = 1\n",
    "n_electrons = 4\n",
    "n_spin_up = 2\n",
    "n_spin_down = n_electrons - n_spin_up\n",
    "n_pairwise = n_electrons**2\n",
    "if not full_pairwise:\n",
    "    n_pairwise -= n_electrons\n",
    "\n",
    "nf_single_in = 4 * n_atoms\n",
    "nf_hidden_single = 128\n",
    "nf_pairwise_in = 4\n",
    "nf_hidden_pairwise = 16\n",
    "nf_intermediate_single = 3*nf_hidden_single + 2*nf_hidden_pairwise\n",
    "\n",
    "n_determinants = 8\n",
    "\n",
    "env_init = 1.\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "# params # in_dim, weight_shape, out_dim, env_init=0.0\n",
    "s_stream_0 = apbi(init(nf_single_in, (nf_single_in, nf_hidden_single), nf_hidden_single))\n",
    "p_stream_0 = apbi(init(nf_pairwise_in, (nf_pairwise_in, nf_hidden_pairwise), nf_hidden_pairwise))\n",
    "\n",
    "s_stream_1 = apbi(init(nf_intermediate_single, (nf_intermediate_single, nf_hidden_single), nf_hidden_single))\n",
    "p_stream_1 = apbi(init(nf_hidden_pairwise, (nf_hidden_pairwise, nf_hidden_pairwise), nf_hidden_pairwise))\n",
    "\n",
    "s_stream_2 = apbi(init(nf_intermediate_single, (nf_intermediate_single, nf_hidden_single), nf_hidden_single))\n",
    "\n",
    "env_w_up = apbi(init(nf_hidden_single, (n_determinants, n_spin_up, nf_hidden_single, 1), 1), axis=2)\n",
    "env_sigma_up  = init(3, (n_determinants, n_spin_up, n_atoms, 3, 3), 3, env_init=env_init)\n",
    "env_pi_up = init(n_atoms, (n_determinants, n_spin_up, n_atoms, 1), 1, env_init=env_init)\n",
    "\n",
    "env_w_down = apbi(init(nf_hidden_single, (n_determinants, n_spin_down, nf_hidden_single, 1), 1), axis=2)\n",
    "env_sigma_down = init(3, (n_determinants, n_spin_down, n_atoms, 3, 3), 3, env_init=env_init)\n",
    "env_pi_down = init(n_atoms, (n_determinants, n_spin_down, n_atoms, 1), 1, env_init=env_init)\n",
    "\n",
    "w_final = init(n_determinants, (1, n_determinants, 1, 1), 1, env_init=env_init/n_determinants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.custom_gradient\n",
    "def safe_norm_grad(x, norm):\n",
    "    # x : (n, ne**2, 3)\n",
    "    # norm : (n, ne**2, 1)\n",
    "    g = x / norm\n",
    "    g = tf.where(tf.math.is_nan(g), tf.zeros_like(g), g)\n",
    "    cache = (x, norm)\n",
    "\n",
    "    def grad_grad(dy):\n",
    "        x, norm = cache\n",
    "        x = tf.expand_dims(x, -1)  # (n, ne**2, 3, 1)\n",
    "        xx = x * tf.transpose(x, perm=(0, 1, 3, 2))  # cross terms\n",
    "        inv_norm = tf.tile(1. / norm, (1, 1, 3))  # (n, ne**2, 3) inf where the ee terms are same e\n",
    "        norm_diag = tf.linalg.diag(inv_norm) # (n, ne**2, 3, 3) # diagonal where the basis vector is the same\n",
    "        gg = norm_diag - xx / tf.expand_dims(norm, -1)**3\n",
    "        gg = tf.reduce_sum(gg, axis=-1)\n",
    "        gg = tf.where(tf.math.is_nan(gg), tf.zeros_like(gg), gg)\n",
    "        tf.debugging.check_numerics(gg, 'gg')\n",
    "        tf.debugging.check_numerics(dy, 'dy')\n",
    "        return dy*gg, None\n",
    "\n",
    "    return g, grad_grad\n",
    "\n",
    "@tf.custom_gradient\n",
    "def safe_norm(x):\n",
    "    norm = tf.norm(x, keepdims=True, axis=-1)\n",
    "    def grad(dy):\n",
    "        g = safe_norm_grad(x, norm)\n",
    "        return dy*g\n",
    "    return norm, grad\n",
    "\n",
    "class Mixer(tk.Model):\n",
    "    def __init__(self, n_electrons, n_single_features, n_pairwise, n_pairwise_features, n_spin_up, n_spin_down, full_pairwise):\n",
    "        super(Mixer, self).__init__()\n",
    "\n",
    "        self.n_spin_up = float(n_spin_up)\n",
    "        self.n_spin_down = float(n_spin_down)\n",
    "\n",
    "        tmp1 = tf.ones((1, n_spin_up, n_single_features), dtype=tf.bool)\n",
    "        tmp2 = tf.zeros((1, n_spin_down, n_single_features), dtype=tf.bool)\n",
    "        self.spin_up_mask = tf.concat((tmp1, tmp2), 1)\n",
    "        self.spin_down_mask = ~self.spin_up_mask\n",
    "\n",
    "        if full_pairwise:\n",
    "            self.pairwise_spin_up_mask, self.pairwise_spin_down_mask = \\\n",
    "                generate_pairwise_masks_full(n_electrons, n_pairwise, n_spin_up, n_spin_down, n_pairwise_features)\n",
    "        else:\n",
    "            self.pairwise_spin_up_mask, self.pairwise_spin_down_mask = \\\n",
    "                generate_pairwise_masks(n_electrons, n_pairwise, n_spin_up, n_spin_down, n_pairwise_features)\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, single, pairwise, n_samples, n_electrons):\n",
    "        # single (n_samples, n_electrons, n_single_features)\n",
    "        # pairwise (n_samples, n_electrons, n_pairwise_features)\n",
    "        spin_up_mask = tf.tile(self.spin_up_mask, (n_samples, 1, 1))\n",
    "        spin_down_mask = tf.tile(self.spin_down_mask, (n_samples, 1, 1))\n",
    "\n",
    "        # --- Single summations\n",
    "        replace = tf.zeros_like(single, dtype=tfDTYPE)\n",
    "        # up\n",
    "        sum_spin_up = tf.where(spin_up_mask, single, replace)\n",
    "        sum_spin_up = tf.reduce_sum(sum_spin_up, 1, keepdims=True) / self.n_spin_up\n",
    "        sum_spin_up = tf.tile(sum_spin_up, (1, n_electrons, 1))\n",
    "        # down\n",
    "        sum_spin_down = tf.where(spin_down_mask, single, replace)\n",
    "        sum_spin_down = tf.reduce_sum(sum_spin_down, 1, keepdims=True) / self.n_spin_down\n",
    "        sum_spin_down = tf.tile(sum_spin_down, (1, n_electrons, 1))\n",
    "\n",
    "        # --- Pairwise summations\n",
    "        sum_pairwise = tf.tile(tf.expand_dims(pairwise, 1), (1, n_electrons, 1, 1))\n",
    "        replace = tf.zeros_like(sum_pairwise, dtype=tfDTYPE)\n",
    "        # up\n",
    "        sum_pairwise_up = tf.where(self.pairwise_spin_up_mask, sum_pairwise, replace)\n",
    "        sum_pairwise_up = tf.reduce_sum(sum_pairwise_up, 2) / self.n_spin_up\n",
    "        # down\n",
    "        sum_pairwise_down = tf.where(self.pairwise_spin_down_mask, sum_pairwise, replace)\n",
    "        sum_pairwise_down = tf.reduce_sum(sum_pairwise_down, 2) / self.n_spin_down\n",
    "\n",
    "        features = tf.concat((single, sum_spin_up, sum_spin_down, sum_pairwise_up, sum_pairwise_down), 2)\n",
    "        return features\n",
    "    \n",
    "\n",
    "class fermi_tf_custom():\n",
    "    def __init__(self, r_atoms):\n",
    "        self.r_atoms = r_atoms\n",
    "        \n",
    "        self.s0 = tf.Variable(s_stream_0)\n",
    "        self.p0 = tf.Variable(p_stream_0)\n",
    "        self.m0 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s1 = tf.Variable(s_stream_1)\n",
    "        self.p1 = tf.Variable(p_stream_1)\n",
    "        self.m1 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s2 = tf.Variable(s_stream_2)\n",
    "        \n",
    "        self.env_w_up = tf.Variable(env_w_up)\n",
    "        self.env_sigma_up = tf.Variable(env_sigma_up)\n",
    "        self.env_pi_up = tf.Variable(env_pi_up)\n",
    "        \n",
    "        self.env_w_down = tf.Variable(env_w_down)\n",
    "        self.env_sigma_down = tf.Variable(env_sigma_down)\n",
    "        self.env_pi_down = tf.Variable(env_pi_down)\n",
    "        \n",
    "        self.w_final = tf.Variable(w_final)\n",
    "        \n",
    "    def __call__(self, samples):\n",
    "        n_samples = samples.shape[0]\n",
    "        ae_vectors = compute_ae_vectors(self.r_atoms, samples, flow=True)\n",
    "        single, pairwise = compute_inputs(samples, n_samples, ae_vectors, n_atoms, n_electrons, full_pairwise)\n",
    "        \n",
    "        # streams \n",
    "        s0 = linear(self.s0, single, n_samples, n_electrons, flow=True)\n",
    "        p0 = linear(self.p0, pairwise, n_samples, n_pairwise, flow=True)\n",
    "        s0m = self.m0(s0, p0, n_samples, n_electrons)\n",
    "        \n",
    "        s1 = linear(self.s1, s0m, n_samples, n_electrons, flow=True)\n",
    "        p1 = linear(self.p1, p0, n_samples, n_pairwise, flow=True)\n",
    "        s1m = self.m1(s1, p1, n_samples, n_electrons)\n",
    "        \n",
    "        s2 = linear(self.s2, s1m, n_samples, n_electrons, flow=True)\n",
    "        \n",
    "        # env inputs\n",
    "        ae_vectors_up, ae_vectors_down = tf.split(ae_vectors, [n_spin_up, n_spin_down], axis=1)\n",
    "        inputs_up, inputs_down = tf.split(s2, [n_spin_up, n_spin_down], axis=1)\n",
    "        \n",
    "        # env_w 'njf,kifs->nkjis'\n",
    "        # env_sigma 'njmv,kimvc->nkjimc'\n",
    "        # env_pi 'njkim,kims->nkjis'\n",
    "        up_dets = env(inputs_up, ae_vectors_up, self.env_w_up, self.env_sigma_up, self.env_pi_up, n_samples, n_spin_up)\n",
    "        down_dets = env(inputs_down, ae_vectors_down, self.env_w_down, self.env_sigma_down, self.env_pi_down, n_samples, n_spin_down)\n",
    "        \n",
    "        log_psi, _, _, _ = log_abs_sum_det(up_dets, down_dets, self.w_final)\n",
    "        \n",
    "        return log_psi\n",
    "    \n",
    "    \n",
    "class fermi_tf():\n",
    "    def __init__(self, r_atoms):\n",
    "        self.r_atoms = r_atoms\n",
    "        \n",
    "        self.s0 = tf.Variable(s_stream_0)\n",
    "        self.p0 = tf.Variable(p_stream_0)\n",
    "        self.m0 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s1 = tf.Variable(s_stream_1)\n",
    "        self.p1 = tf.Variable(p_stream_1)\n",
    "        self.m1 = Mixer(n_electrons, nf_hidden_single, n_pairwise, nf_hidden_pairwise, n_spin_up, n_spin_down, full_pairwise)\n",
    "        \n",
    "        self.s2 = tf.Variable(s_stream_2)\n",
    "        \n",
    "        self.env_w_up = tf.Variable(env_w_up)\n",
    "        self.env_sigma_up = tf.Variable(env_sigma_up)\n",
    "        self.env_pi_up = tf.Variable(env_pi_up)\n",
    "        \n",
    "        self.env_w_down = tf.Variable(env_w_down)\n",
    "        self.env_sigma_down = tf.Variable(env_sigma_down)\n",
    "        self.env_pi_down = tf.Variable(env_pi_down)\n",
    "        \n",
    "        self.w_final = tf.Variable(w_final)\n",
    "        \n",
    "    def __call__(self, samples):\n",
    "        n_samples = samples.shape[0]\n",
    "        ae_vectors = compute_ae_vectors(self.r_atoms, samples, flow=True)\n",
    "        single, pairwise = compute_inputs(samples, n_samples, ae_vectors, n_atoms, n_electrons, full_pairwise)\n",
    "        \n",
    "        # streams \n",
    "        s0 = linear(self.s0, single, n_samples, n_electrons, flow=True)\n",
    "        p0 = linear(self.p0, pairwise, n_samples, n_pairwise, flow=True)\n",
    "        s0m = self.m0(s0, p0, n_samples, n_electrons)\n",
    "        \n",
    "        s1 = linear(self.s1, s0m, n_samples, n_electrons, flow=True)\n",
    "        p1 = linear(self.p1, p0, n_samples, n_pairwise, flow=True)\n",
    "        s1m = self.m1(s1, p1, n_samples, n_electrons)\n",
    "        \n",
    "        s2 = linear(self.s2, s1m, n_samples, n_electrons, flow=True)\n",
    "        \n",
    "        # env inputs\n",
    "        ae_vectors_up, ae_vectors_down = tf.split(ae_vectors, [n_spin_up, n_spin_down], axis=1)\n",
    "        inputs_up, inputs_down = tf.split(s2, [n_spin_up, n_spin_down], axis=1)\n",
    "        \n",
    "        # env_w 'njf,kifs->nkjis'\n",
    "        # env_sigma 'njmv,kimvc->nkjimc'\n",
    "        # env_pi 'njkim,kims->nkjis'\n",
    "        up_dets = env(inputs_up, ae_vectors_up, self.env_w_up, self.env_sigma_up, self.env_pi_up, n_samples, n_spin_up)\n",
    "        down_dets = env(inputs_down, ae_vectors_down, self.env_w_down, self.env_sigma_down, self.env_pi_down, n_samples, n_spin_down)\n",
    "        print('\\n')\n",
    "        up_dets = tf.expand_dims(tf.expand_dims(tf.linalg.det(up_dets), -1), -1)\n",
    "        print(up_dets.shape, self.w_final.shape, (self.w_final * up_dets).shape)\n",
    "        down_dets = tf.expand_dims(tf.expand_dims(tf.linalg.det(down_dets), -1), -1)\n",
    "        \n",
    "#         up_dets = tf.linalg.det(up_dets)\n",
    "#         down_dets = tf.linalg.det(down_dets)\n",
    "        tmp = self.w_final * up_dets * down_dets \n",
    "        print(tmp.shape)\n",
    "        log_psi = tf.math.log(tf.abs(tf.reduce_sum(tmp, axis=1)))\n",
    "        print('\\n')\n",
    "        return log_psi\n",
    "    \n",
    "\n",
    "def slogdet_keepdim(tensor):\n",
    "    sign, tensor_out = tf.linalg.slogdet(tensor)\n",
    "    tensor_out = tf.reshape(tensor_out, (*tensor_out.shape, 1, 1))\n",
    "    sign = tf.reshape(sign, (*sign.shape, 1, 1))\n",
    "    return sign, tensor_out\n",
    "\n",
    "\n",
    "def generate_gamma(s):\n",
    "    n_egvs = s.shape[2]\n",
    "    gamma = [tf.reduce_prod(s[:, :, :i], axis=-1) * tf.reduce_prod(s[:, :, i+1:], axis=-1) for i in range(n_egvs-1)]\n",
    "    gamma.append(tf.reduce_prod(s[:, :, :-1], axis=-1))\n",
    "    gamma = tf.stack(gamma, axis=2)\n",
    "    gamma = tf.expand_dims(gamma, axis=2)\n",
    "    return gamma\n",
    "\n",
    "\n",
    "def first_derivative_det(A):\n",
    "    with tf.device(\"/cpu:0\"):  # this is incredible stupid /// its actually not\n",
    "        s, u, v = tf.linalg.svd(A, full_matrices=False)\n",
    "    v_t = tf.linalg.matrix_transpose(v)\n",
    "    gamma = generate_gamma(s)\n",
    "    sign = (tf.linalg.det(u) * tf.linalg.det(v))[..., None, None]\n",
    "    out = sign * ((u * gamma) @ v_t)\n",
    "    return out, (s, u, v_t, sign)\n",
    "\n",
    "\n",
    "def generate_p(s):\n",
    "    n_samples, n_k, n_dim = s.shape\n",
    "    new_shape = (1, 1, 1, n_dim, n_dim)\n",
    "    s = s[..., None, None]\n",
    "    s = tf.tile(s, new_shape)\n",
    "    mask = np.ones(s.shape, dtype=np.bool)\n",
    "    for i in range(n_dim):\n",
    "        for j in range(n_dim):\n",
    "            mask[..., i, i, j] = False\n",
    "            mask[..., j, i, j] = False\n",
    "    mask = tf.convert_to_tensor(mask)\n",
    "    s = tf.where(mask, s, tf.ones_like(s, dtype=s.dtype))\n",
    "    s_prod = tf.reduce_prod(s, axis=-3)\n",
    "    s_prod = tf.linalg.set_diag(s_prod, tf.zeros((s_prod.shape[:-1]), dtype=s.dtype))\n",
    "    return s_prod\n",
    "\n",
    "\n",
    "def second_derivative_det(A, C_dash, *A_cache):\n",
    "\n",
    "    s, u, v_t, sign = A_cache  # decompose the cache\n",
    "\n",
    "    M = v_t @ tf.linalg.matrix_transpose(C_dash) @ u\n",
    "\n",
    "    p = generate_p(s)\n",
    "\n",
    "    sgn = tf.math.sign(sign)\n",
    "\n",
    "    m_jj = tf.linalg.diag_part(M)\n",
    "    xi = -M * p\n",
    "\n",
    "    xi_diag = p @ tf.expand_dims(m_jj, -1)\n",
    "    xi = tf.linalg.set_diag(xi, tf.squeeze(xi_diag, -1))\n",
    "    return sgn * u @ xi @ v_t\n",
    "\n",
    "\n",
    "def k_sum(x):\n",
    "    return tf.reduce_sum(x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def matrix_sum(x):\n",
    "    return tf.reduce_sum(x, axis=[-2, -1], keepdims=True)\n",
    "\n",
    "\n",
    "def _log_abs_sum_det_second_order(a_dash, b_dash, w_dash, *cache):\n",
    "\n",
    "    a, b, w, unshifted_exp, sign_unshifted_sum, sign_a, logdet_a, sign_b, logdet_b, log_psi, \\\n",
    "    sign_u, ddeta, ddeta_cache, ddetb, ddetb_cache, dfddeta, dfddetb, da, db, dw = cache\n",
    "\n",
    "    dfddeta_w = dfddeta / w\n",
    "    dfddetb_w = dfddetb / w\n",
    "\n",
    "    ddeta_sum = matrix_sum(a_dash * ddeta)\n",
    "    da_sum = matrix_sum(da * a_dash)\n",
    "    ddetb_sum = matrix_sum(b_dash * ddetb)\n",
    "    db_sum = matrix_sum(db * b_dash)\n",
    "    a_sum = k_sum(dfddeta * ddeta_sum)\n",
    "    b_sum = k_sum(dfddetb * ddetb_sum)\n",
    "\n",
    "    # Compute second deriviate of f wrt to w\n",
    "    d2w = w_dash * -dw * k_sum(dw)\n",
    "\n",
    "    # compute deriviate of df/da wrt to w\n",
    "    dadw = -dw * k_sum(da_sum)\n",
    "    dadw += dfddeta_w * ddeta_sum  # i=j\n",
    "\n",
    "    # compute derivative of df/db wrt to w\n",
    "    dbdw = -dw * k_sum(db_sum)\n",
    "    dbdw += dfddetb_w * ddetb_sum  # i=j\n",
    "\n",
    "    # Compute second derivative of f wrt to a\n",
    "    d2a = -da * a_sum\n",
    "    d2a += dfddeta * second_derivative_det(a, a_dash, *ddeta_cache)  # i=j\n",
    "    # Compute derivative of df/db wrt to a\n",
    "    dbda = -da * b_sum\n",
    "    dbda += ddeta * sign_u * tf.exp(-log_psi) * w * ddetb_sum  # i=j\n",
    "    # Compute derivative of df/dw wrt to a\n",
    "    dwda = w_dash * -da * k_sum(dw)\n",
    "    dwda += w_dash * da / w  # i=j\n",
    "\n",
    "    # Compute second derivative of f wrt to b\n",
    "    d2b = -db * b_sum\n",
    "    d2b += dfddetb * second_derivative_det(b, b_dash, *ddetb_cache)  # i=j\n",
    "    # Compute derivative of df/da wrt to b\n",
    "    dadb = -db * a_sum\n",
    "    dadb += ddetb * sign_u * tf.exp(-log_psi) * w * ddeta_sum  # i=j\n",
    "    # Compute derivative of df/dw wrt to b\n",
    "    dwdb = w_dash * -db * k_sum(dw)\n",
    "    dwdb += w_dash * db / w  # i=j\n",
    "\n",
    "    return (d2a + dbda + dwda), (d2b + dadb + dwdb), \\\n",
    "           (d2w + dadw + dbdw), \\\n",
    "           None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "def _log_abs_sum_det_fwd(a, b, w):\n",
    "\n",
    "    # Take the slogdet of all k determinants\n",
    "    sign_a, logdet_a = slogdet_keepdim(a)\n",
    "    sign_b, logdet_b = slogdet_keepdim(b)\n",
    "\n",
    "    x = logdet_a + logdet_b\n",
    "    xmax = tf.math.reduce_max(x, axis=1, keepdims=True)\n",
    "\n",
    "    unshifted_exp = sign_a * sign_b * tf.exp(x)\n",
    "    unshifted_exp_w = w * unshifted_exp\n",
    "    sign_unshifted_sum = tf.math.sign(tf.reduce_sum(unshifted_exp_w, axis=1, keepdims=True))\n",
    "\n",
    "    exponent = x - xmax\n",
    "    shifted_exp = sign_a * sign_b * tf.exp(exponent)\n",
    "\n",
    "    u = w * shifted_exp\n",
    "    u_sum = tf.reduce_sum(u, axis=1, keepdims=True)\n",
    "    sign_shifted_sum = tf.math.sign(u_sum)\n",
    "    log_psi = tf.math.log(tf.math.abs(u_sum)) + xmax\n",
    "\n",
    "    sensitivities = tf.exp(-log_psi) * sign_unshifted_sum\n",
    "\n",
    "    return log_psi, sign_unshifted_sum, unshifted_exp, sensitivities, \\\n",
    "           (a, b, w, unshifted_exp, sign_unshifted_sum, sign_a, logdet_a, sign_b, logdet_b, log_psi)\n",
    "\n",
    "\n",
    "def _log_abs_sum_det_first_order(*fwd_cache):\n",
    "\n",
    "    a, b, w, unshifted_exp, sign_unshifted_sum, sign_a, logdet_a, sign_b, logdet_b, log_psi = fwd_cache\n",
    "\n",
    "    ddeta, ddeta_cache = first_derivative_det(a)\n",
    "    ddetb, ddetb_cache = first_derivative_det(b)\n",
    "\n",
    "    dfddeta = w * sign_unshifted_sum * sign_b * tf.exp(logdet_b - log_psi)\n",
    "    dfddetb = w * sign_unshifted_sum * sign_a * tf.exp(logdet_a - log_psi)\n",
    "\n",
    "    da = dfddeta * ddeta\n",
    "    db = dfddetb * ddetb\n",
    "    dw = sign_unshifted_sum * unshifted_exp * tf.exp(-log_psi)\n",
    "    \n",
    "    return (da, db, dw), (sign_unshifted_sum, ddeta, ddeta_cache, ddetb, ddetb_cache, dfddeta, dfddetb, da, db, dw)\n",
    "\n",
    "\n",
    "@tf.custom_gradient\n",
    "def first_order_gradient(a_unused, b_unused, w_unused, *fwd_cache):\n",
    "\n",
    "    (da, db, dw), first_order_cache = _log_abs_sum_det_first_order(*fwd_cache)\n",
    "    return (da, db, dw), \\\n",
    "           lambda a_dash, b_dash, w_dash: _log_abs_sum_det_second_order(\n",
    "               a_dash, b_dash, w_dash, *fwd_cache, *first_order_cache)\n",
    "\n",
    "@tf.custom_gradient\n",
    "def log_abs_sum_det(a, b, w):\n",
    "\n",
    "    log_psi, sign, act, sens, fwd_cache = _log_abs_sum_det_fwd(a, b, w)\n",
    "\n",
    "    def _first_order_grad(dy, dsg, _, __):\n",
    "        da, db, dw = first_order_gradient(a, b, w, *fwd_cache)\n",
    "\n",
    "        return dy * da, dy * db, tf.reduce_sum(dy * dw, axis=0, keepdims=True)\n",
    "\n",
    "    return (log_psi, sign, act, sens), _first_order_grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ps(ls):\n",
    "    for t, n in ls:\n",
    "        print(n, t.shape)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/maxiwelian/anaconda3/envs/aqua/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:253: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "\n",
      "\n",
      "(100, 8, 1, 1) (1, 8, 1, 1) (100, 8, 1, 1)\n",
      "(100, 8, 1, 1)\n",
      "\n",
      "\n",
      "(100, 1, 1) (100, 1, 1, 1)\n",
      "3.730349362740526e-16\n"
     ]
    }
   ],
   "source": [
    "samples = np.random.normal(size=(n_samples, n_electrons, 3)).astype(npDTYPE)\n",
    "r_atoms = np.zeros(shape=(1,3)).astype(npDTYPE)\n",
    "\n",
    "r_atoms = tf.convert_to_tensor(r_atoms, dtype=tfDTYPE)\n",
    "samples = tf.convert_to_tensor(samples, dtype=tfDTYPE)\n",
    "\n",
    "ftf_custom = fermi_tf_custom(r_atoms)\n",
    "psi = ftf_custom(samples)\n",
    "\n",
    "ftf = fermi_tf(r_atoms)\n",
    "tcpsi = ftf(samples)\n",
    "\n",
    "print(tcpsi.shape, psi.shape)\n",
    "print(compare_tf_tensors(tf.squeeze(tcpsi), tf.squeeze(psi)))\n",
    "\n",
    "# for a, b in zip(psi, tcpsi):\n",
    "#     print(a.numpy(), b.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(100, 8, 1, 1) (1, 8, 1, 1) (100, 8, 1, 1)\n",
      "(100, 8, 1, 1)\n",
      "\n",
      "\n",
      "4.039320372641201e-14\n",
      "87.1913604402419\n"
     ]
    }
   ],
   "source": [
    "g_custom, gg_custom = tflaplacian(ftf_custom, samples)\n",
    "\n",
    "g, gg = tflaplacian(ftf, samples)\n",
    "\n",
    "print(compare_tf_tensors(tf.squeeze(g_custom), tf.squeeze(g)))\n",
    "print(compare_tf_tensors(tf.squeeze(gg_custom), tf.squeeze(gg)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.diag(tc.tensor([[1,2], [4, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
