{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch as tc\n",
    "import numpy as np\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from torch.autograd import grad as tcgrad\n",
    "DTYPE = tf.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch(tensors):\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        new_tensor = tc.tensor(tensor.numpy(), requires_grad=True)\n",
    "        new_tensors.append(new_tensor)\n",
    "    return new_tensors\n",
    "\n",
    "def fn(A, B, W):\n",
    "    return tc.log(tc.abs(tc.sum(W * A.det().view(-1,n_k,1,1) * B.det().view(-1,n_k,1,1), axis=1, keepdim=True)))\n",
    "\n",
    "def compute_ae_vectors(r_atoms, r_electrons):\n",
    "    # ae_vectors (n_samples, n_electrons, n_atoms, 3)\n",
    "    r_atoms = tf.expand_dims(r_atoms, 1)\n",
    "    r_electrons = tf.expand_dims(r_electrons, 2)\n",
    "    ae_vectors = r_electrons - r_atoms\n",
    "    return ae_vectors\n",
    "\n",
    "def compute_relative_vectors(v1, v2):\n",
    "    relative_vectors = v1.unsqueeze(2) - v2.unsqueeze(1)\n",
    "    return relative_vectors\n",
    "\n",
    "def compare_tensors(ptorch, tflow):\n",
    "    ptorch = ptorch.detach().numpy()\n",
    "    tflow = tflow.numpy()\n",
    "    mask = np.isclose(ptorch, tflow, rtol=0.0, atol=1e-4).astype(np.float32)\n",
    "    return np.mean(mask)\n",
    "\n",
    "def compare_tf_tensors(tflow1, tflow2,rtol=1e-05, atol=1e-08): #\n",
    "    tflow1 = tflow1.numpy()\n",
    "    tflow2 = tflow2.numpy()\n",
    "    mask = np.isclose(tflow1, tflow2).astype(np.float32)\n",
    "    return np.mean(mask) \n",
    "\n",
    "ct = compare_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slogdet_keepdim(tensor):\n",
    "    sign, tensor_out = tf.linalg.slogdet(tensor)\n",
    "    tensor_out = tf.reshape(tensor_out, (*tensor_out.shape, 1, 1))\n",
    "    sign = tf.reshape(sign, (*sign.shape, 1, 1))\n",
    "    return sign, tensor_out\n",
    "\n",
    "\n",
    "def generate_gamma(s):\n",
    "    n_egvs = s.shape[2]\n",
    "    gamma = [tf.reduce_prod(s[:, :, :i], axis=-1) * tf.reduce_prod(s[:, :, i+1:], axis=-1) for i in range(n_egvs-1)]\n",
    "    gamma.append(tf.reduce_prod(s[:, :, :-1], axis=-1))\n",
    "    gamma = tf.stack(gamma, axis=2)\n",
    "    gamma = tf.expand_dims(gamma, axis=2)\n",
    "    return gamma\n",
    "\n",
    "\n",
    "def first_derivative_det(A):\n",
    "    with tf.device(\"/cpu:0\"):  # this is incredible stupid /// its actually not\n",
    "        s, u, v = tf.linalg.svd(A, full_matrices=False)\n",
    "    # s, u, v = tf.linalg.svd(A, full_matrices=False)\n",
    "    v_t = tf.linalg.matrix_transpose(v)\n",
    "    gamma = generate_gamma(s)\n",
    "    sign = (tf.linalg.det(u) * tf.linalg.det(v))[..., None, None]\n",
    "    out = sign * ((u * gamma) @ v_t)\n",
    "    return out, (s, u, v_t, sign)\n",
    "\n",
    "\n",
    "def generate_p(s):\n",
    "    \"\"\"\n",
    "    :param s:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_samples, n_k, n_dim = s.shape\n",
    "    new_shape = (1, 1, 1, n_dim, n_dim)\n",
    "    s = s[..., None, None]\n",
    "    s = tf.tile(s, new_shape)\n",
    "    mask = np.ones(s.shape, dtype=np.bool)\n",
    "    for i in range(n_dim):\n",
    "        for j in range(n_dim):\n",
    "            mask[..., i, i, j] = False\n",
    "            mask[..., j, i, j] = False\n",
    "    mask = tf.convert_to_tensor(mask)\n",
    "    s = tf.where(mask, s, tf.ones_like(s, dtype=s.dtype))\n",
    "    s_prod = tf.reduce_prod(s, axis=-3)\n",
    "    s_prod = tf.linalg.set_diag(s_prod, tf.zeros((s_prod.shape[:-1]), dtype=s.dtype))\n",
    "    return s_prod\n",
    "\n",
    "\n",
    "def second_derivative_det(A, C_dash, *A_cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param A:\n",
    "    :param C_dash:\n",
    "    :param A_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # This function computes the second order derivative of detA wrt to A\n",
    "    # A matrix\n",
    "    # C_bar backward sensitivity\n",
    "    # A_cache cached values returned by grad_det(A)\n",
    "    s, u, v_t, sign = A_cache  # decompose the cache\n",
    "\n",
    "    M = v_t @ tf.linalg.matrix_transpose(C_dash) @ u\n",
    "\n",
    "    p = generate_p(s)\n",
    "\n",
    "    sgn = tf.math.sign(sign)\n",
    "\n",
    "    m_jj = tf.linalg.diag_part(M)\n",
    "    xi = -M * p\n",
    "\n",
    "    xi_diag = p @ tf.expand_dims(m_jj, -1)\n",
    "    xi = tf.linalg.set_diag(xi, tf.squeeze(xi_diag, -1))\n",
    "    return sgn * u @ xi @ v_t\n",
    "\n",
    "\n",
    "def k_sum(x):\n",
    "    return tf.reduce_sum(x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def matrix_sum(x):\n",
    "    return tf.reduce_sum(x, axis=[-2, -1], keepdims=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _log_abs_sum_det_fwd(a, b, w):\n",
    "    a = tf.stop_gradient(a)\n",
    "    b = tf.stop_gradient(b)\n",
    "    w = tf.stop_gradient(w)\n",
    "    \"\"\"\n",
    "\n",
    "    :param a:\n",
    "    :param b:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Take the slogdet of all k determinants\n",
    "    sign_a, logdet_a = slogdet_keepdim(a)\n",
    "    sign_b, logdet_b = slogdet_keepdim(b)\n",
    "\n",
    "    x = logdet_a + logdet_b\n",
    "    xmax = tf.math.reduce_max(x, axis=1, keepdims=True)\n",
    "\n",
    "    unshifted_exp = sign_a * sign_b * tf.exp(x)\n",
    "    unshifted_exp_w = w * unshifted_exp\n",
    "    sign_unshifted_sum = tf.math.sign(tf.reduce_sum(unshifted_exp_w, axis=1, keepdims=True))\n",
    "\n",
    "    exponent = x - xmax\n",
    "    shifted_exp = sign_a * sign_b * tf.exp(exponent)\n",
    "\n",
    "    u = w * shifted_exp\n",
    "    u_sum = tf.reduce_sum(u, axis=1, keepdims=True)\n",
    "    sign_shifted_sum = tf.math.sign(u_sum)\n",
    "    log_psi = tf.math.log(tf.math.abs(u_sum)) + xmax\n",
    "\n",
    "    # Both of these derivations appear to be valid\n",
    "    # activations = shifted_exp\n",
    "    # sensitivities = sign_unshifted_sum * tf.exp(-log_psi)\n",
    "    # dw = sign_unshifted_sum * sign_a * sign_b * tf.exp(x-log_psi)\n",
    "    #\n",
    "    # return log_psi, sign_shifted_sum, activations, sensitivities, \\\n",
    "    #           (a, b, w, unshifted_exp, sign_unshifted_sum, dw, sign_a, logdet_a, sign_b, logdet_b, log_psi)\n",
    "\n",
    "    sensitivities = tf.exp(-log_psi) * sign_unshifted_sum\n",
    "    # sensitivities = tf.exp(xmax-log_psi) * sign_shifted_sum\n",
    "\n",
    "    dw = sign_unshifted_sum * sign_a * sign_b * tf.exp(x - log_psi)\n",
    "\n",
    "    return log_psi, sign_unshifted_sum, unshifted_exp, sensitivities, \\\n",
    "           (a, b, w, unshifted_exp, sign_unshifted_sum, dw, sign_a, logdet_a, sign_b, logdet_b, log_psi)\n",
    "\n",
    "# @tf.function\n",
    "def _log_abs_sum_det_first_order(*fwd_cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param fwd_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a, b, w, unshifted_exp, sign_unshifted_sum, dw, sign_a, logdet_a, sign_b, logdet_b, log_psi = fwd_cache\n",
    "\n",
    "    ddeta, ddeta_cache = first_derivative_det(a)\n",
    "    ddetb, ddetb_cache = first_derivative_det(b)\n",
    "\n",
    "    dfddeta = w * sign_unshifted_sum * sign_b * tf.exp(logdet_b - log_psi)\n",
    "    dfddetb = w * sign_unshifted_sum * sign_a * tf.exp(logdet_a - log_psi)\n",
    "\n",
    "    da = dfddeta * ddeta\n",
    "    db = dfddetb * ddetb\n",
    "\n",
    "    return (da, db, dw), (sign_unshifted_sum, ddeta, ddeta_cache, ddetb, ddetb_cache, dfddeta, dfddetb, da, db, dw)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def _log_abs_sum_det_second_order(a_dash, b_dash, w_dash, *cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a_dash:\n",
    "    :param b_dash:\n",
    "    :param w_dash:\n",
    "    :param cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a, b, w, unshifted_exp, sign_unshifted_sum, _, sign_a, logdet_a, sign_b, logdet_b, log_psi, \\\n",
    "    sign_u, ddeta, ddeta_cache, ddetb, ddetb_cache, dfddeta, dfddetb, da, db, dw = cache\n",
    "\n",
    "    dfddeta_w = dfddeta / w\n",
    "    dfddetb_w = dfddetb / w\n",
    "\n",
    "    ddeta_sum = matrix_sum(a_dash * ddeta)\n",
    "    da_sum = matrix_sum(da * a_dash)\n",
    "    ddetb_sum = matrix_sum(b_dash * ddetb)\n",
    "    db_sum = matrix_sum(db * b_dash)\n",
    "    a_sum = k_sum(dfddeta * ddeta_sum)\n",
    "    b_sum = k_sum(dfddetb * ddetb_sum)\n",
    "\n",
    "    # Compute second deriviate of f wrt to w\n",
    "    d2w = w_dash * -dw * k_sum(dw)\n",
    "\n",
    "    # compute deriviate of df/da wrt to w\n",
    "    dadw = -dw * k_sum(da_sum)\n",
    "    dadw += dfddeta_w * ddeta_sum  # i=j\n",
    "\n",
    "    # compute derivative of df/db wrt to w\n",
    "    dbdw = -dw * k_sum(db_sum)\n",
    "    dbdw += dfddetb_w * ddetb_sum  # i=j\n",
    "\n",
    "    # Compute second derivative of f wrt to a\n",
    "    d2a = -da * a_sum\n",
    "    d2a += dfddeta * second_derivative_det(a, a_dash, *ddeta_cache)  # i=j\n",
    "    # Compute derivative of df/db wrt to a\n",
    "    dbda = -da * b_sum\n",
    "    dbda += ddeta * sign_u * tf.exp(-log_psi) * w * ddetb_sum  # i=j\n",
    "    # Compute derivative of df/dw wrt to a\n",
    "    dwda = w_dash * -da * k_sum(dw)\n",
    "    dwda += w_dash * da / w  # i=j\n",
    "\n",
    "    # Compute second derivative of f wrt to b\n",
    "    d2b = -db * b_sum\n",
    "    d2b += dfddetb * second_derivative_det(b, b_dash, *ddetb_cache)  # i=j\n",
    "    # Compute derivative of df/da wrt to b\n",
    "    dadb = -db * a_sum\n",
    "    dadb += ddetb * sign_u * tf.exp(-log_psi) * w * ddeta_sum  # i=j\n",
    "    # Compute derivative of df/dw wrt to b\n",
    "    dwdb = w_dash * -db * k_sum(dw)\n",
    "    dwdb += w_dash * db / w  # i=j\n",
    "\n",
    "    return (d2a + dbda + dwda), (d2b + dadb + dwdb), \\\n",
    "           (d2w + dadw + dbdw), \\\n",
    "           None, None, None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "# @tf.autograph.experimental.do_not_convert\n",
    "# @tf.function\n",
    "@tf.custom_gradient\n",
    "def first_order_gradient(a_unused, b_unused, w_unused, *fwd_cache):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a_unused:\n",
    "    :param b_unused:\n",
    "    :param w_unused:\n",
    "    :param fwd_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    (da, db, dw), first_order_cache = _log_abs_sum_det_first_order(*fwd_cache)\n",
    "    return (da, db, dw), \\\n",
    "           lambda a_dash, b_dash, w_dash: _log_abs_sum_det_second_order(\n",
    "               a_dash, b_dash, w_dash, *fwd_cache, *first_order_cache)\n",
    "\n",
    "\n",
    "# @tf.autograph.experimental.do_not_convert\n",
    "# @tf.function\n",
    "@tf.custom_gradient\n",
    "def log_abs_sum_det(a, b, w):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a:\n",
    "    :param b:\n",
    "    :param w:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    log_psi, sign, act, sens, fwd_cache = _log_abs_sum_det_fwd(a, b, w)\n",
    "\n",
    "    def _first_order_grad(dy, dsg, _, __):\n",
    "        da, db, dw = first_order_gradient(a, b, w, *fwd_cache)\n",
    "\n",
    "        # print('dy', dy)\n",
    "        return dy * da, dy * db, tf.reduce_sum(dy * dw, axis=0, keepdims=True)\n",
    "    # tf.reduce_sum(dy * dw, axis=0, keepdims=True)\n",
    "\n",
    "    return (log_psi, sign, act, sens), _first_order_grad\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "n_k = 10\n",
    "ndim = 4\n",
    "A = tf.random.normal((n_samples, n_k, ndim, ndim), dtype=DTYPE)\n",
    "B = tf.random.normal((n_samples, n_k, ndim, ndim), dtype=DTYPE)\n",
    "W = tf.random.normal((1,n_k,1,1), dtype=DTYPE)\n",
    "\n",
    "A_tc, B_tc, W_tc = convert_to_torch([A, B, W])\n",
    "\n",
    "with tf.GradientTape(True) as g:\n",
    "    g.watch(A)\n",
    "    g.watch(W)\n",
    "    g.watch(B)\n",
    "    with tf.GradientTape(True) as gg:\n",
    "        gg.watch(A)\n",
    "        gg.watch(B)\n",
    "        gg.watch(W)\n",
    "        log_psi,_, _, _ = log_abs_sum_det(A, B, W)\n",
    "    grad_1_A = gg.gradient(log_psi, A)\n",
    "    grad_1_B = gg.gradient(log_psi, B)\n",
    "    grad_1_W = gg.gradient(log_psi, W)\n",
    "grad_2_A = g.gradient(grad_1_A, A)\n",
    "\n",
    "log_psi_tc = fn(A_tc, B_tc, W_tc)\n",
    "\n",
    "# first order\n",
    "grad_1_Atc = tcgrad(log_psi_tc.sum(), A_tc, retain_graph=True, create_graph=True)[0]\n",
    "grad_1_Btc = tcgrad(log_psi_tc.sum(), B_tc, retain_graph=True, create_graph=True)[0]\n",
    "grad_1_Wtc = tcgrad(log_psi_tc.sum(), W_tc, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "# second order\n",
    "grad_2_Atc = tcgrad(grad_1_Atc.sum(), A_tc)[0]\n",
    "\n",
    "# validation\n",
    "validate_log_psi = compare_tensors(log_psi_tc, log_psi)\n",
    "print(validate_log_psi)\n",
    "validate_gradW = compare_tensors(grad_1_Wtc, grad_1_W)\n",
    "print(validate_gradW)\n",
    "\n",
    "validate_grad_1_A = compare_tensors(grad_1_Atc, grad_1_A) \n",
    "print(validate_grad_1_A)\n",
    "\n",
    "validate_grad_2_A = compare_tensors(grad_2_Atc, grad_2_A)\n",
    "print(validate_grad_2_A)\n",
    "\n",
    "# if validate_grad1 < 1:\n",
    "#     log_psi_tc = log_psi_tc.reshape(-1)\n",
    "#     log_psi = tf.reshape(log_psi, (-1,))\n",
    "#     for el1, el2 in zip(log_psi_tc, log_psi):\n",
    "#         print(abs(el1.detach().numpy()-el2.numpy()))\n",
    "#     print(log_psi_tc, log_psi)\n",
    "    \n",
    "#     print('determinants: ', A_tc.det(), B_tc.det())\n",
    "# print(compare_tensors(grad_1_tc, grad_1))\n",
    "\n",
    "# print('B: ', ct(B_tc, B))\n",
    "# print('ae_vectors:', compare_tensors(ae_vectors_tc, ae_vectors))\n",
    "# print('exponential: ', compare_tensors(expo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a9eb2ca64802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_2' is not defined"
     ]
    }
   ],
   "source": [
    "grad_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fermi_tf(r_atoms, re, w1, w_up, b_up, Sigma_up, Pi_up, w_down, b_down, Sigma_down, Pi_down):\n",
    "    ae_vectors = compute_ae_vectors(r_atoms, re)\n",
    "    r2 = tf.einsum('fv,niv->nif',w1,re)\n",
    "\n",
    "    # Envelopes\n",
    "    ae_vectors_up = ae_vectors[:,:n_up,...]\n",
    "    r_up = r2[:,:n_up,:]\n",
    "    factor = tf.einsum('njf,kfi->nkji', r_up, w_up)\n",
    "    factor = factor + b_up\n",
    "    exp = tf.einsum('kimvc,njmv->nkijmc', Sigma_up, ae_vectors_up)\n",
    "    exponential = tf.exp(-tf.norm(exp, axis=-1))\n",
    "    exp = tf.einsum('nkijm,kim->nkij', exponential, Pi_up)\n",
    "    A = factor * exp\n",
    "\n",
    "    ae_vectors_down = ae_vectors[:,n_up:,...]\n",
    "    r_down = r2[:,n_up:,:]\n",
    "    factor = tf.einsum('njf,kfi->nkji', r_down, w_down)\n",
    "    factor = factor + b_down\n",
    "    exp = tf.einsum('kimvc,njmv->nkijmc', Sigma_down, ae_vectors_down)\n",
    "    exponential = tf.exp(-tf.norm(exp, axis=-1))\n",
    "    exp = tf.einsum('nkijm,kim->nkij', exponential, Pi_down)\n",
    "    B = factor * exp\n",
    "    \n",
    "    return A, B\n",
    "    \n",
    "    \n",
    "def fermi_tc(r_atoms_tc, re_tc, w1_tc,  w_up_tc, b_up_tc, Sigma_up_tc, Pi_up_tc, \\\n",
    "              w_down_tc, b_down_tc, Sigma_down_tc, Pi_down_tc):\n",
    "    ae_vectors_tc = compute_relative_vectors(re_tc,r_atoms_tc)\n",
    "    # ### Torch\n",
    "    r2_tc = tc.einsum('fv,niv->nif',[w1_tc,re_tc])\n",
    "\n",
    "    # Envelopes\n",
    "    ae_vectors_up_tc = ae_vectors_tc[:,:n_up,...]\n",
    "    r_up_tc = r2_tc[:,:n_up,:]\n",
    "    factor = tc.einsum('njf,kfi->nkji', [r_up_tc, w_up_tc])\n",
    "    factor = factor + b_up_tc\n",
    "    exp = tc.einsum('kimvc,njmv->nkijmc', [Sigma_up_tc, ae_vectors_up_tc])\n",
    "    exponential = tc.exp(-exp.norm(dim=-1))\n",
    "    exp = tc.einsum('nkijm,kim->nkij', [exponential, Pi_up_tc])\n",
    "    A_tc = factor * exp\n",
    "\n",
    "    ae_vectors_down_tc = ae_vectors_tc[:,n_up:,...]\n",
    "    r_down_tc = r2_tc[:,n_up:,:]\n",
    "    factor = tc.einsum('njf,kfi->nkji', [r_down_tc, w_down_tc])\n",
    "    factor = factor + b_down_tc\n",
    "    exp = tc.einsum('kimvc,njmv->nkijmc', [Sigma_down_tc, ae_vectors_down_tc])\n",
    "    exponential = tc.exp(-exp.norm(dim=-1))\n",
    "    exp = tc.einsum('nkijm,kim->nkij', [exponential, Pi_down_tc])\n",
    "    B_tc = factor * exp\n",
    "    return A_tc, B_tc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logabssumdet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0ddb334c190c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         A, B = fermi_tf(r_atoms, re, w1,  w_up, b_up, Sigma_up, Pi_up, \\\n\u001b[1;32m     34\u001b[0m                          w_down, b_down, Sigma_down, Pi_down)\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlog_psi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogabssumdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mgrad_1_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_psi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logabssumdet' is not defined"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "n_k = 10\n",
    "n_f = 20\n",
    "n_up = 2\n",
    "n_e = 4\n",
    "n_down = n_e - n_up\n",
    "n_atom = 1 \n",
    "\n",
    "### Parameters\n",
    "# First\n",
    "r_atoms = tf.zeros((n_samples, n_atom, 3), dtype=DTYPE)\n",
    "re = tf.random.normal((n_samples, n_e, 3), dtype=DTYPE) #* 100.\n",
    "w1 = tf.random.normal((n_f, 3), dtype=DTYPE)\n",
    "# up\n",
    "w_up = tf.random.normal((n_k, n_f, n_up), dtype=DTYPE)\n",
    "b_up = tf.random.normal((n_k, n_up, 1), dtype=DTYPE)\n",
    "Sigma_up = tf.random.normal((n_k, n_up, n_atom, 3, 3), dtype=DTYPE)\n",
    "Pi_up = tf.random.normal((n_k, n_up, n_atom), dtype=DTYPE)\n",
    "# down\n",
    "w_down = tf.random.normal((n_k, n_f, n_down), dtype=DTYPE)\n",
    "b_down = tf.random.normal((n_k, n_down, 1), dtype=DTYPE)\n",
    "Sigma_down = tf.random.normal((n_k, n_down, n_atom, 3, 3), dtype=DTYPE)\n",
    "Pi_down = tf.random.normal((n_k, n_down, n_atom), dtype=DTYPE)\n",
    "# Weights\n",
    "W = tf.random.normal((1, n_k, 1, 1), dtype=DTYPE)\n",
    "\n",
    "with tf.GradientTape(True) as g:\n",
    "    g.watch(W)\n",
    "    g.watch(re)\n",
    "    with tf.GradientTape(True) as gg:\n",
    "        gg.watch(re)\n",
    "        gg.watch(W)\n",
    "        A, B = fermi_tf(r_atoms, re, w1,  w_up, b_up, Sigma_up, Pi_up, \\\n",
    "                         w_down, b_down, Sigma_down, Pi_down)\n",
    "        log_psi,_ = log_abs_sum_det(A, B, W)\n",
    "\n",
    "    grad_1_A = gg.gradient(log_psi, A)\n",
    "    grad_1_B = gg.gradient(log_psi, B)\n",
    "    grad_1_W = gg.gradient(log_psi, W)\n",
    "    grad_1_re = gg.gradient(log_psi, re)\n",
    "    grads_1_re = tf.reshape(grad_1_re, (-1, n_e*3))\n",
    "    grads_1_re = [grads_1_re[..., i] for i in range(grads_1_re.shape[-1])]\n",
    "    \n",
    "grad_2_re = g.gradient(grads_1_re[0], re)\n",
    "grad_2_A = g.gradient(grad_1_A, A)\n",
    "\n",
    "# print(grad_2_re)\n",
    "# print(grad_2)\n",
    "\n",
    "### Torch\n",
    "re_tc, w1_tc, w_up_tc, b_up_tc, w_down_tc, b_down_tc, W_tc = \\\n",
    "convert_to_torch([re, w1, w_up, b_up, w_down, b_down, W])\n",
    "Sigma_up_tc, Pi_up_tc, Sigma_down_tc, Pi_down_tc = \\\n",
    "convert_to_torch([Sigma_up, Pi_up, Sigma_down, Pi_down])\n",
    "r_atoms_tc = convert_to_torch([r_atoms])[0]\n",
    "\n",
    "\n",
    "A_tc, B_tc = fermi_tc(r_atoms_tc, re_tc, w1_tc, w_up_tc, b_up_tc, Sigma_up_tc, Pi_up_tc, \\\n",
    "              w_down_tc, b_down_tc, Sigma_down_tc, Pi_down_tc)\n",
    "log_psi_tc = fn(A_tc, B_tc, W_tc)\n",
    "\n",
    "grad_1_Atc = tcgrad(log_psi_tc.sum(), A_tc, retain_graph=True, create_graph=True)[0]\n",
    "grad_2_Atc = tcgrad(grad_1_Atc.sum(), A_tc, retain_graph=True)[0]\n",
    "grad_1_Btc = tcgrad(log_psi_tc.sum(), B_tc, retain_graph=True)[0]\n",
    "grad_1_Wtc = tcgrad(log_psi_tc.sum(), W_tc, retain_graph=True)[0]\n",
    "grad_1_retc = tcgrad(log_psi_tc.sum(), re_tc, retain_graph=True, create_graph=True)[0]\n",
    "grads_1_retc = grad_1_retc.view(-1,3*n_e)\n",
    "grad_2_retc = tcgrad(grads_1_retc[:,0].sum(), re_tc, retain_graph=True)[0]\n",
    "\n",
    "grad_1 = tcgrad(log_psi_tc.sum(), A_tc)\n",
    "\n",
    "validate_log_psi = compare_tensors(log_psi_tc, log_psi)\n",
    "print(validate_log_psi)\n",
    "validate_gradW = compare_tensors(grad_1_Wtc, grad_1_W)\n",
    "print(validate_gradW)\n",
    "\n",
    "print(compare_tensors(grad_1_Atc, grad_1_A))\n",
    "print(compare_tensors(grad_1_Btc, grad_1_B))\n",
    "\n",
    "validate_re = ct(grad_1_retc, grad_1_re)\n",
    "print('re grad: ', validate_re)\n",
    "\n",
    "validate_grads_re = ct(grads_1_retc[:,0], grads_1_re[0])\n",
    "print('re grads: ', validate_grads_re)\n",
    "\n",
    "validate_2_re = ct(grad_2_retc, grad_2_re)\n",
    "print('re grad: ', validate_2_re)\n",
    "\n",
    "validate_2_A = ct(grad_2_Atc, grad_2_A)\n",
    "print('re grad: ', validate_2_A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_1_retc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed4ba4ef576d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad_1_retc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_1_retc' is not defined"
     ]
    }
   ],
   "source": [
    "grad_1_retc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_1_re_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d3d2125f7568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad_1_re_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_1_re_loss' is not defined"
     ]
    }
   ],
   "source": [
    "grad_1_re_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_2_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cdaef393a778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad_2_A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_2_A' is not defined"
     ]
    }
   ],
   "source": [
    "grad_2_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_2_retc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-57a75faa86af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad_2_retc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_2_retc' is not defined"
     ]
    }
   ],
   "source": [
    "grad_2_retc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- compare nico to new \n",
    "\n",
    "# --- outputs the same\n",
    "\n",
    "# --- grad1 the same (ABW)\n",
    "\n",
    "# --- grad1 the same (r)\n",
    "\n",
    "# --- grad2 the same (ABW)\n",
    "\n",
    "# --- grad2 the same (r)\n",
    "\n",
    "# --- speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
